from __future__ import annotations

import base64
import io
import json
import re
import textwrap
import zipfile
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import streamlit as st

from src.core.config import load_config
from src.core.config_validator import validate_config, CodeAssistantConfig
from src.core.llm_client import llm_chat, build_llm_config
from src.core.logger import setup_logger
from src.core.orchestrator import Orchestrator
from src.core.subproc import run_cmd
from src.features.review.notebook import extract_code_cells
from src.features.review.rule_plugin import get_registry
from src.reporting.latex_builder import build_latex_report
from src.reporting.pdf_builder import build_pdf_report
from src.reporting.report_builder import build_markdown_report

def _inject_css() -> None:
    # åŠ è½½èƒŒæ™¯å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
    bg_image_base64 = ""
    bg_image_path = Path("background.png")
    if bg_image_path.exists():
        try:
            bg_image_base64 = base64.b64encode(bg_image_path.read_bytes()).decode()
        except Exception:
            pass  # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤èƒŒæ™¯
    
    # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡ç”Ÿæˆä¸åŒçš„èƒŒæ™¯æ ·å¼
    if bg_image_base64:
        hero_background = f"""
  background: linear-gradient(var(--hero-overlay), var(--hero-overlay)),
              url('data:image/png;base64,{bg_image_base64}') center/cover no-repeat;
"""
    else:
        hero_background = """
  background: var(--panel);
"""
    
    st.markdown(
        f"""
<style>
@import url("https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono:wght@400;600&display=swap");

:root {{
  --bg: #f7f7f8;
  --panel: #ffffff;
  --ink: #0f172a;
  --muted: #64748b;
  --accent: #10a37f;
  --accent-strong: #0c8b6b;
  --accent-soft: #22c28f;
  --accent-glow: rgba(16, 163, 127, 0.2);
  --border: #e2e8f0;
  --shadow: 0 16px 40px rgba(15, 23, 42, 0.08);
  --sidebar-bg: #fafafa;
  --input-bg: #ffffff;
  --input-border: #dbe3ee;
  --code-bg: #f1f5f9;
  --code-border: #e2e8f0;
  --hero-overlay: rgba(255, 255, 255, 0.3);
  --hero-text-shadow: 2px 2px 4px rgba(255, 255, 255, 0.8);
  --hero-subtext-shadow: 1px 1px 2px rgba(255, 255, 255, 0.8);
  --app-glow-1: radial-gradient(1000px 500px at 10% 0%, rgba(16, 163, 127, 0.18) 0%, transparent 55%);
  --app-glow-2: radial-gradient(900px 450px at 90% 0%, rgba(52, 211, 153, 0.18) 0%, transparent 55%);
  --app-glow-3: radial-gradient(700px 350px at 50% 100%, rgba(26, 188, 156, 0.14) 0%, transparent 50%);
  --sidebar-glow-1: radial-gradient(420px 420px at 0% 70%, rgba(255, 220, 120, 0.28) 0%, transparent 60%);
  --sidebar-glow-2: radial-gradient(320px 320px at 0% 0%, rgba(255, 235, 59, 0.22) 0%, transparent 50%);
}}

@media (prefers-color-scheme: dark) {{
  :root {{
    --bg: #0b0f14;
    --panel: #0f172a;
    --ink: #e2e8f0;
    --muted: #94a3b8;
    --accent: #10b981;
    --accent-strong: #059669;
    --accent-soft: #34d399;
    --accent-glow: rgba(16, 185, 129, 0.35);
    --border: #1f2937;
    --shadow: 0 18px 50px rgba(0, 0, 0, 0.45);
    --sidebar-bg: #0c1422;
    --input-bg: #0b1220;
    --input-border: #233146;
    --code-bg: #0b1220;
    --code-border: #233146;
    --hero-overlay: rgba(11, 18, 32, 0.6);
    --hero-text-shadow: 0 4px 12px rgba(0, 0, 0, 0.6);
    --hero-subtext-shadow: 0 2px 8px rgba(0, 0, 0, 0.6);
    --app-glow-1: radial-gradient(1000px 500px at 10% 0%, rgba(16, 185, 129, 0.18) 0%, transparent 55%);
    --app-glow-2: radial-gradient(900px 450px at 90% 0%, rgba(56, 189, 248, 0.12) 0%, transparent 55%);
    --app-glow-3: radial-gradient(700px 350px at 50% 100%, rgba(234, 179, 8, 0.12) 0%, transparent 50%);
    --sidebar-glow-1: radial-gradient(420px 420px at 0% 70%, rgba(16, 185, 129, 0.18) 0%, transparent 60%);
    --sidebar-glow-2: radial-gradient(320px 320px at 0% 0%, rgba(56, 189, 248, 0.12) 0%, transparent 50%);
  }}
}}

html[data-theme="dark"],
body[data-theme="dark"] {{
  --bg: #0b0f14;
  --panel: #0f172a;
  --ink: #e2e8f0;
  --muted: #94a3b8;
  --accent: #10b981;
  --accent-strong: #059669;
  --accent-soft: #34d399;
  --accent-glow: rgba(16, 185, 129, 0.35);
  --border: #1f2937;
  --shadow: 0 18px 50px rgba(0, 0, 0, 0.45);
  --sidebar-bg: #0c1422;
  --input-bg: #0b1220;
  --input-border: #233146;
  --code-bg: #0b1220;
  --code-border: #233146;
  --hero-overlay: rgba(11, 18, 32, 0.6);
  --hero-text-shadow: 0 4px 12px rgba(0, 0, 0, 0.6);
  --hero-subtext-shadow: 0 2px 8px rgba(0, 0, 0, 0.6);
  --app-glow-1: radial-gradient(1000px 500px at 10% 0%, rgba(16, 185, 129, 0.18) 0%, transparent 55%);
  --app-glow-2: radial-gradient(900px 450px at 90% 0%, rgba(56, 189, 248, 0.12) 0%, transparent 55%);
  --app-glow-3: radial-gradient(700px 350px at 50% 100%, rgba(234, 179, 8, 0.12) 0%, transparent 50%);
  --sidebar-glow-1: radial-gradient(420px 420px at 0% 70%, rgba(16, 185, 129, 0.18) 0%, transparent 60%);
  --sidebar-glow-2: radial-gradient(320px 320px at 0% 0%, rgba(56, 189, 248, 0.12) 0%, transparent 50%);
}}

.stApp {{
  background: var(--app-glow-1), var(--app-glow-2), var(--app-glow-3), var(--bg);
  color: var(--ink);
  font-family: "Space Grotesk", "IBM Plex Sans", sans-serif;
}}

div.block-container {{
  padding-top: 2rem;
  padding-bottom: 4rem;
  max-width: 1180px;
  animation: rise 0.6s ease-out;
}}

section[data-testid="stSidebar"] {{
  background: var(--sidebar-glow-1), var(--sidebar-glow-2), var(--sidebar-bg);
  border-right: 1px solid var(--border);
}}

section[data-testid="stSidebar"] h1,
section[data-testid="stSidebar"] h2,
section[data-testid="stSidebar"] h3,
section[data-testid="stSidebar"] label,
section[data-testid="stSidebar"] span {{
  color: var(--ink);
}}

section[data-testid="stSidebar"] .stTextInput input,
section[data-testid="stSidebar"] .stTextArea textarea {{
  background: var(--input-bg);
  border: 1px solid var(--input-border);
  color: var(--ink);
  border-radius: 12px;
  padding: 0.55rem 0.75rem;
}}

section[data-testid="stSidebar"] .stTextInput input::placeholder,
section[data-testid="stSidebar"] .stTextArea textarea::placeholder {{
  color: var(--muted);
}}

.stButton button {{
  width: 100%;
  border-radius: 999px;
  border: 1px solid transparent;
  background: linear-gradient(135deg, var(--accent), var(--accent-soft));
  color: #ffffff;
  font-weight: 600;
  padding: 0.6rem 1.2rem;
  transition: transform 0.2s ease, box-shadow 0.2s ease, background 0.2s ease;
}}

.stButton button:hover {{
  transform: translateY(-1px);
  box-shadow: 0 10px 22px var(--accent-glow);
  background: linear-gradient(135deg, var(--accent-strong), var(--accent));
}}

.stDownloadButton button {{
  width: 100%;
  border-radius: 999px;
  border: 1px solid transparent;
  background: linear-gradient(135deg, var(--accent), var(--accent-soft)) !important;
  color: #ffffff !important;
  font-weight: 600;
  padding: 0.6rem 1.2rem;
  transition: transform 0.2s ease, box-shadow 0.2s ease, background 0.2s ease;
}}

.stDownloadButton button:hover {{
  transform: translateY(-1px);
  box-shadow: 0 10px 22px var(--accent-glow);
  background: linear-gradient(135deg, var(--accent-strong), var(--accent)) !important;
}}

h1, h2, h3 {{
  letter-spacing: -0.02em;
}}

.hero {{
  padding: 1.5rem 1.75rem;{hero_background}
  border: 1px solid var(--border);
  border-radius: 18px;
  box-shadow: var(--shadow);
  margin-bottom: 1.5rem;
  position: relative;
}}

.hero-title {{
  font-size: 2.1rem;
  font-weight: 700;
  position: relative;
  z-index: 1;
  text-shadow: var(--hero-text-shadow);
}}

.hero-subtitle {{
  margin-top: 0.35rem;
  color: var(--muted);
  font-size: 1rem;
  position: relative;
  z-index: 1;
  text-shadow: var(--hero-subtext-shadow);
}}

div[data-testid="column"] > div {{
  background: var(--panel);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 1rem;
  box-shadow: var(--shadow);
}}

div[data-testid="column"] {{
  animation: rise 0.6s ease-out;
}}

div[data-testid="column"]:nth-child(1) {{ animation-delay: 0.05s; }}
div[data-testid="column"]:nth-child(2) {{ animation-delay: 0.1s; }}
div[data-testid="column"]:nth-child(3) {{ animation-delay: 0.15s; }}

div[data-testid="stMetric"] {{
  background: var(--panel);
  border: 1px solid var(--border);
  border-radius: 14px;
  padding: 0.75rem;
  box-shadow: var(--shadow);
}}

code, pre {{
  font-family: "JetBrains Mono", "Fira Code", monospace;
  background: var(--code-bg);
  border: 1px solid var(--code-border);
  border-radius: 10px;
}}

@keyframes rise {{
  from {{ opacity: 0; transform: translateY(8px); }}
  to {{ opacity: 1; transform: translateY(0); }}
}}
</style>
""",
        unsafe_allow_html=True,
    )


def _ensure_dirs(out_dir: str) -> None:
    Path(out_dir).mkdir(parents=True, exist_ok=True)


def _parse_repo_inputs(text: str) -> List[str]:
    items = []
    for raw in text.splitlines():
        raw = raw.strip()
        if not raw or raw.startswith("#"):
            continue
        items.append(raw)
    return items


def _is_repo_root(path: Path) -> bool:
    markers = [".git", "pyproject.toml", "setup.py", "setup.cfg", "requirements.txt"]
    return any((path / m).exists() for m in markers)


def _expand_local_repos(path: Path) -> List[Path]:
    if not path.exists():
        return []
    if path.is_file():
        return [path]
    subrepos = [p for p in path.iterdir() if p.is_dir() and _is_repo_root(p)]
    if len(subrepos) >= 2:
        return sorted(subrepos, key=lambda p: p.name.lower())
    if _is_repo_root(path):
        return [path]
    if subrepos:
        return sorted(subrepos, key=lambda p: p.name.lower())
    return [path]


def _unique_name(name: str, used: Dict[str, int]) -> str:
    base = name or "repo"
    if base not in used:
        used[base] = 1
        return base
    used[base] += 1
    return f"{base}-{used[base]}"


def _is_github_url(value: str) -> bool:
    value = value.strip().lower()
    return value.startswith("https://github.com/") or value.startswith("http://github.com/") or value.startswith(
        "git@github.com:"
    )


def _github_slug(url: str) -> Optional[str]:
    url = url.strip()
    if url.startswith("git@github.com:"):
        path = url.split("git@github.com:", 1)[1]
    else:
        parsed = urlparse(url)
        if parsed.netloc not in {"github.com", "www.github.com"}:
            return None
        path = parsed.path.lstrip("/")
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2:
        return None
    owner, repo = parts[0], parts[1]
    repo = repo.replace(".git", "")
    return f"{owner}__{repo}"


def _resolve_repo_input(repo_input: str, cache_dir: str) -> Optional[str]:
    repo_input = repo_input.strip()
    if not repo_input:
        st.error("ä»“åº“è·¯å¾„æˆ– URL ä¸ºç©ºã€‚")
        return None
    if not _is_github_url(repo_input):
        return repo_input
    slug = _github_slug(repo_input)
    if not slug:
        st.error("æ— æ•ˆçš„ GitHub ä»“åº“ URLã€‚")
        return None
    cache_root = Path(cache_dir).expanduser().resolve()
    dest = cache_root / slug
    if dest.exists():
        st.info(f"ä½¿ç”¨ç¼“å­˜ä»“åº“ï¼š{dest}")
        return str(dest)
    cache_root.mkdir(parents=True, exist_ok=True)
    st.info(f"æ­£åœ¨å…‹éš† {repo_input} ...")
    res = run_cmd(["git", "clone", "--depth", "1", repo_input, str(dest)])
    if not res["ok"]:
        st.error(f"Git å…‹éš†å¤±è´¥ï¼š{res['stderr'] or res['stdout']}")
        return None
    return str(dest)


def _prepare_cfg(cfg: Dict[str, Any], test_out: Optional[Path]) -> Dict[str, Any]:
    cfg = dict(cfg)
    if test_out is not None:
        cfg.setdefault("testgen", {})
        cfg["testgen"] = dict(cfg["testgen"])
        cfg["testgen"]["output_dir"] = str(test_out)
    return cfg


def _resolve_repo_inputs(repo_text: str, cache_dir: str) -> List[Dict[str, str]]:
    items = _parse_repo_inputs(repo_text)
    repos: List[Dict[str, str]] = []
    used: Dict[str, int] = {}
    for raw in items:
        if _is_github_url(raw):
            resolved = _resolve_repo_input(raw, cache_dir)
            if not resolved:
                continue
            slug = _github_slug(raw) or Path(resolved).name
            name = _unique_name(slug, used)
            repos.append({"name": name, "path": resolved, "source": raw})
            continue
        p = Path(raw).expanduser()
        if not p.exists():
            st.error(f"Repository path not found: {raw}")
            continue
        for local in _expand_local_repos(p):
            name = _unique_name(local.name or "repo", used)
            repos.append({"name": name, "path": str(local.resolve()), "source": raw})
    return repos


def _markdown_to_text(md_text: str) -> str:
    lines = []
    for line in md_text.splitlines():
        if line.startswith("#"):
            line = line.lstrip("#").strip().upper()
        line = line.replace("**", "").replace("`", "")
        lines.append(line)
    return "\n".join(lines)


def _build_pdf_from_markdown(md_text: str) -> Optional[bytes]:
    try:
        from reportlab.lib.pagesizes import letter
        from reportlab.lib.units import inch
        from reportlab.pdfgen import canvas
    except Exception:
        return None
    text = _markdown_to_text(md_text)
    buf = io.BytesIO()
    c = canvas.Canvas(buf, pagesize=letter)
    _, height = letter
    margin = 0.75 * inch
    x = margin
    y = height - margin
    line_height = 12
    wrap_width = 110
    for line in text.splitlines():
        chunks = textwrap.wrap(line, width=wrap_width, replace_whitespace=False, drop_whitespace=False) or [""]
        for chunk in chunks:
            if y <= margin:
                c.showPage()
                y = height - margin
            c.drawString(x, y, chunk)
            y -= line_height
    c.save()
    return buf.getvalue()


def _render_pdf_preview(pdf_bytes: bytes) -> None:
    b64 = base64.b64encode(pdf_bytes).decode("ascii")
    html = (
        f'<iframe src="data:application/pdf;base64,{b64}" '
        'width="100%" height="720" style="border: 1px solid #e2e8f0; border-radius: 14px;"></iframe>'
    )
    st.markdown(html, unsafe_allow_html=True)


def _compile_latex(tex_path: Path) -> Optional[Path]:
    pdf_path = tex_path.with_suffix(".pdf")
    res = run_cmd(
        ["xelatex", "-interaction=nonstopmode", "-halt-on-error", tex_path.name],
        cwd=str(tex_path.parent),
    )
    if res["ok"] and pdf_path.exists():
        res2 = run_cmd(
            ["xelatex", "-interaction=nonstopmode", "-halt-on-error", tex_path.name],
            cwd=str(tex_path.parent),
        )
        if pdf_path.exists():
            return pdf_path
    res = run_cmd(["tectonic", tex_path.name], cwd=str(tex_path.parent))
    if res["ok"] and pdf_path.exists():
        return pdf_path
    return None


def _load_report_sources(out_dir: str, state):
    review = state.get("last_review")
    testgen = state.get("last_testgen")
    review_path = Path(out_dir) / "review.json"
    testgen_path = Path(out_dir) / "testgen.json"
    if review is None and review_path.exists():
        try:
            review = json.loads(review_path.read_text(encoding="utf-8"))
        except Exception:
            review = None
    if testgen is None and testgen_path.exists():
        try:
            testgen = json.loads(testgen_path.read_text(encoding="utf-8"))
        except Exception:
            testgen = None
    return review, testgen


def _make_pdf_bytes(review, testgen, md_text: str) -> Optional[bytes]:
    pdf_bytes = build_pdf_report(review, testgen)
    if pdf_bytes is not None:
        return pdf_bytes
    return _build_pdf_from_markdown(md_text)


def _write_report(out_dir: str, review, testgen) -> tuple[Path, Optional[Path]]:
    _ensure_dirs(out_dir)
    md = build_markdown_report(review, testgen)
    report_path = Path(out_dir) / "report.md"
    report_path.write_text(md, encoding="utf-8")
    tex_path = Path(out_dir) / "report.tex"
    tex_path.write_text(build_latex_report(review, testgen), encoding="utf-8")
    pdf_path = _compile_latex(tex_path)
    if pdf_path:
        return report_path, pdf_path
    pdf_bytes = _make_pdf_bytes(review, testgen, md)
    if pdf_bytes:
        fallback_path = Path(out_dir) / "report.pdf"
        fallback_path.write_bytes(pdf_bytes)
        return report_path, fallback_path
    return report_path, None


def _existing_report_path(out_dir: str, state) -> Optional[Path]:
    last = state.get("last_report_path")
    if last and Path(last).exists():
        return Path(last)
    candidate = Path(out_dir) / "report.md"
    if candidate.exists():
        return candidate
    return None


def _plot_counts(values: List[str], title: str) -> None:
    values = [v for v in values if v is not None]
    if not values:
        st.info("æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤ºã€‚")
        return
    counts = Counter(values)
    items = sorted(counts.items(), key=lambda kv: (-kv[1], str(kv[0])))
    x = [k for k, _ in items]
    y = [v for _, v in items]
    try:
        import plotly.graph_objects as go

        fig = go.Figure(data=[go.Bar(x=x, y=y)])
        fig.update_layout(title=title, yaxis_title="Count")
        st.plotly_chart(fig, width="stretch")
        return
    except Exception:
        pass
    st.markdown(f"**{title}**")
    max_count = max(y) if y else 0
    lines = []
    for k, v in items:
        bar_len = 0 if max_count == 0 else int(v / max_count * 20)
        bar = "#" * bar_len
        lines.append(f"{k}: {bar} ({v})")
    st.text("\n".join(lines))


def _show_findings_table(rows) -> None:
    try:
        import pandas as pd

        if getattr(pd, "DataFrame", None) is None or getattr(pd, "__version__", None) is None:
            raise RuntimeError("pandas not usable")
    except Exception:
        st.json(rows)
        return
    st.dataframe(rows, width="stretch")

def _truncate_text(text: str, limit: int = 12000) -> str:
    text = text or ""
    if len(text) <= limit:
        return text
    return text[:limit] + "\n...[truncated]..."


def _extract_json_block(text: str) -> Optional[Dict[str, Any]]:
    if not text:
        return None
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None
    try:
        return json.loads(text[start : end + 1])
    except Exception:
        return None


def _normalize_plan(raw: Any) -> List[Dict[str, str]]:
    plan: List[Dict[str, str]] = []
    if isinstance(raw, dict):
        raw = raw.get("plan") or raw.get("steps") or raw.get("changes") or []
    if isinstance(raw, list):
        for idx, item in enumerate(raw, start=1):
            if isinstance(item, str):
                plan.append({"id": str(idx), "file": "", "summary": item})
            elif isinstance(item, dict):
                summary = (
                    item.get("summary")
                    or item.get("change")
                    or item.get("desc")
                    or item.get("title")
                    or ""
                )
                file = item.get("file") or item.get("path") or ""
                plan.append({"id": str(item.get("id", idx)), "file": str(file), "summary": str(summary)})
    return plan


def _fallback_plan(text: str) -> List[Dict[str, str]]:
    lines: List[str] = []
    for raw in (text or "").splitlines():
        line = raw.strip()
        if not line:
            continue
        if line[0].isdigit() or line.startswith("-"):
            line = line.lstrip("- ")
            line = line.lstrip("0123456789.")
            line = line.strip()
        if line:
            lines.append(line)
    return [{"id": str(i + 1), "file": "", "summary": line} for i, line in enumerate(lines[:10])]


def _normalize_recommendations(raw: Any) -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    if isinstance(raw, dict):
        raw = raw.get("projects") or raw.get("recommendations") or raw.get("items") or []
    if isinstance(raw, list):
        for item in raw:
            if not isinstance(item, dict):
                continue
            name = str(item.get("name") or "").strip()
            url = str(item.get("url") or "").strip()
            why = str(item.get("why") or item.get("pros") or item.get("summary") or "").strip()
            if name or url or why:
                items.append({"name": name, "url": url, "why": why})
    return items


def _format_file_context(files: List[Dict[str, str]]) -> str:
    blocks: List[str] = []
    for item in files:
        path = item.get("path") or ""
        content = item.get("content") or ""
        blocks.append(f"FILE: {path}\n```python\n{content}\n```")
    return "\n\n".join(blocks)


def _collect_context_files(
    repo_root: Path,
    review: Optional[Dict[str, Any]],
    max_files: int = 6,
    max_chars: int = 4000,
) -> List[Dict[str, str]]:
    if not review:
        return []
    findings = review.get("findings", []) or []
    seen: set[Path] = set()
    out: List[Dict[str, str]] = []
    for finding in findings:
        file_path = finding.get("file")
        if not file_path:
            continue
        file_path = str(file_path).split("#", 1)[0]
        p = Path(file_path)
        if not p.is_absolute():
            p = repo_root / file_path
        try:
            p = p.resolve()
        except Exception:
            continue
        if repo_root not in p.parents and p != repo_root:
            continue
        if not p.exists() or p in seen:
            continue
        if p.suffix == ".ipynb":
            cells = extract_code_cells(p)
            content = "\n\n".join([f"# cell {idx}\n{code}" for idx, code in cells])
        else:
            content = p.read_text(encoding="utf-8", errors="ignore")
        content = content.strip()
        if not content:
            continue
        if len(content) > max_chars:
            content = content[:max_chars] + "\n...[truncated]..."
        rel = str(p.relative_to(repo_root)).replace("\\", "/")
        out.append({"path": rel, "content": content})
        seen.add(p)
        if len(out) >= max_files:
            break
    return out


def _llm_ready(cfg: Dict[str, Any]) -> tuple[bool, str]:
    llm_cfg = build_llm_config(cfg)
    if not llm_cfg.get("api_key") and not llm_cfg.get("allow_empty_key"):
        env_name = llm_cfg.get("api_key_env") or "OPENAI_API_KEY"
        return False, f"Missing API key. Set {env_name} or llm.api_key."
    return True, ""


def _apply_llm_changes(
    repo_root: Path,
    files: List[Dict[str, Any]],
    allow_new: bool = False,
) -> tuple[List[Path], List[str]]:
    allowed_exts = {".py", ".md", ".yaml", ".yml", ".txt", ".json", ".toml", ".ini", ".cfg", ".ipynb"}
    changed: List[Path] = []
    skipped: List[str] = []
    for item in files:
        path_val = str(item.get("path") or "").strip()
        content = item.get("content")
        if not path_val or content is None:
            skipped.append(path_val or "<empty>")
            continue
        p = Path(path_val)
        if p.is_absolute():
            try:
                rel = p.relative_to(repo_root)
            except Exception:
                skipped.append(path_val)
                continue
        else:
            rel = p
        target = (repo_root / rel).resolve()
        if repo_root not in target.parents and target != repo_root:
            skipped.append(path_val)
            continue
        if target.suffix not in allowed_exts:
            skipped.append(path_val)
            continue
        if not target.exists() and not allow_new:
            skipped.append(path_val)
            continue
        target.parent.mkdir(parents=True, exist_ok=True)
        new_text = str(content)
        if target.exists():
            try:
                old_text = target.read_text(encoding="utf-8", errors="ignore")
                if old_text == new_text:
                    continue
            except Exception:
                pass
        target.write_text(new_text, encoding="utf-8")
        changed.append(target)
    return changed, skipped


def _build_changes_zip(files: List[Path], repo_root: Path) -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for path in files:
            try:
                arc = str(path.relative_to(repo_root)).replace("\\", "/")
                data = path.read_text(encoding="utf-8", errors="ignore")
                zf.writestr(arc, data)
            except Exception:
                continue
    return buf.getvalue()

def _llm_generate_plan(report_text: str, files_context: str, cfg: Dict[str, Any]) -> Dict[str, Any]:
    system_msg = (
        "You are a senior Python engineer. Using the report and file context, "
        "propose a minimal fix plan. Only refer to the provided files. "
        "Return strict JSON: {\"plan\":[{\"id\":1,\"file\":\"path\",\"summary\":\"...\"}],\"notes\":\"...\"}."
    )
    user_msg = f"REPORT:\n{report_text}\n\nFILES:\n{files_context}\n"
    res = llm_chat([
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ], cfg)
    if not res.get("ok"):
        return {"ok": False, "error": res.get("error") or "LLM error"}
    raw = res.get("text", "")
    data = _extract_json_block(raw)
    plan = _normalize_plan(data) if data else []
    if not plan:
        plan = _fallback_plan(raw)
    return {"ok": True, "plan": plan, "raw": raw}


def _llm_generate_changes(
    report_text: str,
    plan: List[Dict[str, str]],
    files_context: str,
    cfg: Dict[str, Any],
) -> Dict[str, Any]:
    plan_json = json.dumps(plan, ensure_ascii=True, indent=2)
    system_msg = (
        "You are a senior Python engineer. Apply the plan using the provided files. "
        "Return strict JSON: {\"files\":[{\"path\":\"path\",\"content\":\"<full file text>\"}],\"notes\":\"...\"}. "
        "Only include files you change; keep everything else untouched."
    )
    user_msg = (
        f"PLAN:\n{plan_json}\n\nREPORT:\n{report_text}\n\nFILES:\n{files_context}\n"
    )
    res = llm_chat([
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ], cfg)
    if not res.get("ok"):
        return {"ok": False, "error": res.get("error") or "LLM error"}
    raw = res.get("text", "")
    data = _extract_json_block(raw)
    files = []
    if isinstance(data, dict):
        files = data.get("files") or data.get("changes") or []
    if not isinstance(files, list):
        files = []
    return {"ok": True, "files": files, "raw": raw}


def _llm_generate_recommendations(report_text: str, cfg: Dict[str, Any]) -> Dict[str, Any]:
    system_msg = (
        "You are a software advisor. Based on the report, recommend 3-5 relevant projects. "
        "Return strict JSON: {\"projects\":[{\"name\":\"...\",\"url\":\"https://...\",\"why\":\"...\"}]}."
    )
    user_msg = f"REPORT:\n{report_text}\n"
    res = llm_chat([
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ], cfg)
    if not res.get("ok"):
        return {"ok": False, "error": res.get("error") or "LLM error"}
    raw = res.get("text", "")
    data = _extract_json_block(raw)
    recs = _normalize_recommendations(data) if data else []
    if not recs:
        recs = _normalize_recommendations(_extract_json_block(raw) or {})
    return {"ok": True, "projects": recs, "raw": raw}

def main() -> None:
    st.set_page_config(page_title="ä»£ç åŠ©æ‰‹", layout="wide")
    _inject_css()

    st.markdown(
        """
<div class="hero">
  <div class="hero-title">ä»£ç åŠ©æ‰‹ </div>
  <div class="hero-subtitle">æ•°æ®ç§‘å­¦å®¡æŸ¥ã€æµ‹è¯•ç”Ÿæˆå’ŒæŠ¥å‘Šä¸€ç«™å¼å®Œæˆ</div>
</div>
""",
        unsafe_allow_html=True,
    )
    
    # æ·»åŠ é¡µé¢é€‰æ‹©å™¨
    page = st.radio(
        "å¯¼èˆª",
        ["ğŸ  ä¸»å·¥ä½œåŒº", "ğŸ“š è§„åˆ™æ–‡æ¡£", "âš™ï¸ é…ç½®ç®¡ç†"],
        horizontal=True,
        label_visibility="collapsed"
    )
    
    # è§„åˆ™æ–‡æ¡£é¡µé¢
    if page == "ğŸ“š è§„åˆ™æ–‡æ¡£":
        st.markdown("## ğŸ“š å¯ç”¨è§„åˆ™æ–‡æ¡£")
        
        # DS åŸºç¡€è§„åˆ™
        with st.expander("ğŸ¯ Data Science Basic Rules (7 ä¸ª)", expanded=True):
            st.markdown("""
            #### DS_RANDOM_SEED
            **ä¸¥é‡æ€§**: Medium  
            **æè¿°**: æ£€æµ‹ä½¿ç”¨éšæœºæ€§ä½†æœªè®¾ç½®ç§å­  
            **ç¤ºä¾‹**:
            ```python
            # âŒ é”™è¯¯
            import random
            x = random.random()
            
            # âœ… æ­£ç¡®
            import random
            random.seed(42)
            x = random.random()
            ```
            
            #### DS_SKLEARN_RANDOM_STATE
            **ä¸¥é‡æ€§**: Medium  
            **æè¿°**: sklearn éšæœºç»„ä»¶ç¼ºå°‘ random_state å‚æ•°  
            **ç¤ºä¾‹**:
            ```python
            # âŒ é”™è¯¯
            clf = RandomForestClassifier(n_estimators=100)
            
            # âœ… æ­£ç¡®
            clf = RandomForestClassifier(n_estimators=100, random_state=42)
            ```
            
            #### DS_LEAKAGE_FIT_BEFORE_SPLIT
            **ä¸¥é‡æ€§**: High  
            **æè¿°**: fit_transform åœ¨ train_test_split ä¹‹å‰å¯èƒ½å¯¼è‡´æ•°æ®æ³„æ¼  
            **ç¤ºä¾‹**:
            ```python
            # âŒ é”™è¯¯
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            X_train, X_test = train_test_split(X_scaled)
            
            # âœ… æ­£ç¡®
            X_train, X_test = train_test_split(X)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)
            ```
            
            #### DS_PIPELINE_SUGGEST
            **ä¸¥é‡æ€§**: Medium  
            **æè¿°**: ç¼©æ”¾å™¨æœªåœ¨ Pipeline ä¸­ä½¿ç”¨  
            
            #### DS_MODEL_PICKLE_UNSAFE
            **ä¸¥é‡æ€§**: High  
            **æè¿°**: ä½¿ç”¨ pickle åºåˆ—åŒ–æ¨¡å‹ä¸å®‰å…¨  
            **å»ºè®®**: ä½¿ç”¨ joblib.dump() æˆ– ONNX å¯¼å‡º
            
            #### DS_HYPERPARAMS_HARDCODED
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: æ¨¡å‹è¶…å‚æ•°ç¡¬ç¼–ç   
            **å»ºè®®**: ä½¿ç”¨ GridSearchCV æˆ–é…ç½®æ–‡ä»¶
            
            #### DS_PANDAS_ITERROWS / DS_PANDAS_APPLY_AXIS1
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: pandas ä½æ•ˆæ“ä½œ  
            **å»ºè®®**: ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
            """)
        
        # DS é«˜çº§è§„åˆ™
        with st.expander("ğŸš€ Data Science Advanced Rules (5 ä¸ª)", expanded=True):
            st.markdown("""
            #### DS_FEATURE_SELECTION_NO_NESTED_CV
            **ä¸¥é‡æ€§**: Medium  
            **æè¿°**: ç‰¹å¾é€‰æ‹©åæœªä½¿ç”¨åµŒå¥—äº¤å‰éªŒè¯å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ  
            
            #### DS_IMBALANCE_NOT_IN_PIPELINE
            **ä¸¥é‡æ€§**: High  
            **æè¿°**: é‡‡æ ·æ–¹æ³•ï¼ˆSMOTEï¼‰æœªåœ¨ Pipeline ä¸­å¯èƒ½å¯¼è‡´æ•°æ®æ³„æ¼  
            
            #### DS_IMBALANCE_UNHANDLED
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: æ¨¡å‹è®­ç»ƒæœªå¤„ç†æ•°æ®ä¸å¹³è¡¡  
            **å»ºè®®**: ä½¿ç”¨ class_weightã€SMOTE æˆ–åˆ†å±‚ CV
            
            #### DS_EVALUATION_INCOMPLETE
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: è¯„ä¼°æŒ‡æ ‡ä¸è¶³  
            **å»ºè®®**: ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ï¼ˆaccuracy, precision, recall, F1, ROC-AUCï¼‰
            """)
        
        # æ’ä»¶è§„åˆ™
        with st.expander("ğŸ”Œ Plugin Rules (4 ä¸ª)", expanded=True):
            st.markdown("""
            #### PY_MUTABLE_DEFAULT_ARG
            **ç±»åˆ«**: Data Science  
            **ä¸¥é‡æ€§**: Medium  
            **æè¿°**: å¯å˜é»˜è®¤å‚æ•°ä¼šè¢«æ‰€æœ‰å‡½æ•°è°ƒç”¨å…±äº«  
            ```python
            # âŒ é”™è¯¯
            def func(items=[]):
                items.append(1)
                return items
            
            # âœ… æ­£ç¡®
            def func(items=None):
                if items is None:
                    items = []
                items.append(1)
                return items
            ```
            
            #### PY_GLOBAL_VARIABLE
            **ç±»åˆ«**: Data Science  
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: è¿‡åº¦ä½¿ç”¨å…¨å±€å˜é‡  
            
            #### PY_RESOURCE_LEAK
            **ç±»åˆ«**: Security  
            **ä¸¥é‡æ€§**: High  
            **æè¿°**: æ–‡ä»¶æœªåœ¨ with è¯­å¥ä¸­ä½¿ç”¨  
            ```python
            # âŒ é”™è¯¯
            f = open('file.txt')
            data = f.read()
            f.close()
            
            # âœ… æ­£ç¡®
            with open('file.txt') as f:
                data = f.read()
            ```
            
            #### PY_LOOP_INVARIANT
            **ç±»åˆ«**: Performance  
            **ä¸¥é‡æ€§**: Low  
            **æè¿°**: å¾ªç¯å†…çš„ä¸å˜è¡¨è¾¾å¼åº”æå–åˆ°å¾ªç¯å¤–  
            """)
        
        st.markdown("---")
        st.info("ğŸ’¡ æç¤º: è¿™äº›è§„åˆ™å¯åœ¨ä¾§è¾¹æ çš„ Advanced Settings ä¸­å¯ç”¨/ç¦ç”¨")
        return
    
    # é…ç½®é¡µé¢
    if page == "âš™ï¸ é…ç½®ç®¡ç†":
        st.markdown("## âš™ï¸ é…ç½®ç®¡ç†")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### å½“å‰é…ç½®")
            cfg_path = st.text_input("é…ç½®æ–‡ä»¶è·¯å¾„", value="config.yaml")
            if Path(cfg_path).exists():
                cfg = load_config(cfg_path, validate=False)
                
                # éªŒè¯é…ç½®
                try:
                    validated = validate_config(cfg)
                    st.success("âœ… é…ç½®éªŒè¯é€šè¿‡")
                    
                    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
                    st.json({
                        "assistant": {
                            "max_files": cfg.get("assistant", {}).get("max_files"),
                            "include_globs": cfg.get("assistant", {}).get("include_globs"),
                        },
                        "review": {
                            "enable_ds_rules": cfg.get("review", {}).get("enable_ds_rules"),
                            "enable_ds_rules_advanced": cfg.get("review", {}).get("enable_ds_rules_advanced"),
                        }
                    })
                except ValueError as e:
                    st.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥:\n{e}")
            else:
                st.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {cfg_path}")
        
        with col2:
            st.markdown("### Pydantic æ•°æ®æ¨¡å‹")
            st.code("""
# é…ç½®æ•°æ®æ¨¡å‹
class AssistantConfig(BaseModel):
    max_files: int = Field(2000, ge=1)
    include_globs: List[str]
    exclude_globs: List[str]

class ReviewConfig(BaseModel):
    enable_ruff: bool = True
    enable_ds_rules: bool = True
    enable_ds_rules_advanced: bool = True
    # ...æ›´å¤šé…ç½®é¡¹

class CodeAssistantConfig(BaseModel):
    assistant: AssistantConfig
    review: ReviewConfig
    testgen: TestGenConfig
    coverage: CoverageConfig
            """, language="python")
        
        st.markdown("---")
        st.info("ğŸ’¡ é…ç½®ä½¿ç”¨ Pydantic è¿›è¡Œç±»å‹éªŒè¯å’Œçº¦æŸæ£€æŸ¥")
        return

    with st.sidebar:
        st.markdown("### ğŸ“‚ å·¥ä½œåŒº")
        cfg_path = st.text_input("é…ç½®æ–‡ä»¶", value="config.yaml")
        repo_input = st.text_area("ä»“åº“è·¯å¾„æˆ– GitHub é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", value="my_repo", height=120)
        cache_dir = st.text_input("GitHub ç¼“å­˜æ–‡ä»¶å¤¹", value="Git_repo")
        st.caption("æ¯è¡Œä¸€ä¸ªä»“åº“ã€‚æœ¬åœ°æ–‡ä»¶å¤¹ä¸­æœ‰å¤šä¸ªä»“åº“æ—¶ä¼šè‡ªåŠ¨å±•å¼€ã€‚")
        out_dir = st.text_input("è¾“å‡ºæ–‡ä»¶å¤¹", value="reports")
        
        st.markdown("---")
        st.markdown("### ğŸ”§ Advanced Settings")
        
        # é«˜çº§è§„åˆ™å¼€å…³
        with st.expander("ğŸ¯ DS è§„åˆ™ï¼ˆæ•°æ®ç§‘å­¦ï¼‰", expanded=False):
            enable_ds_basic = st.checkbox("å¯ç”¨åŸºç¡€ DS è§„åˆ™", value=True, 
                help="éšæœºç§å­ã€æ•°æ®æ³„æ¼ã€Pipeline å»ºè®®ç­‰")
            enable_ds_advanced = st.checkbox("å¯ç”¨é«˜çº§ DS è§„åˆ™", value=False,
                help="âš ï¸ ç‰¹å¾é€‰æ‹©ã€ä¸å¹³è¡¡å¤„ç†ã€è¯„ä¼°æŒ‡æ ‡ç­‰ (è¾ƒæ…¢ï¼Œå»ºè®®ä»…ç”¨äºå°å‹ä»“åº“)")
            
            if enable_ds_advanced:
                st.warning("âš ï¸ é«˜çº§è§„åˆ™ä¼šæ˜¾è‘—å¢åŠ æ‰«ææ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹ä»£ç åº“ã€‚å¦‚æœæ‰«æå¡é¡¿ï¼Œè¯·ç¦ç”¨æ­¤é€‰é¡¹ã€‚")
            
            if enable_ds_basic or enable_ds_advanced:
                st.caption("âœ“ å¤ç°æ€§æ£€æµ‹\nâœ“ æ•°æ®æ³„æ¼æ£€æµ‹\nâœ“ æ¨¡å‹åºåˆ—åŒ–å®‰å…¨\nâœ“ è¶…å‚æ•°ç¡¬ç¼–ç ")
        
        # è§„åˆ™æ’ä»¶ç³»ç»Ÿ
        with st.expander("ğŸ”Œ è§„åˆ™æ’ä»¶", expanded=False):
            # åŠ¨æ€è·å–æ³¨å†Œçš„è§„åˆ™
            try:
                from src.features.review import builtin_rules
                registry = get_registry()
                all_rules = registry.get_all()
                categories = registry.get_categories()
                
                if all_rules:
                    st.caption(f"å·²åŠ è½½ {len(all_rules)} ä¸ªæ’ä»¶è§„åˆ™")
                    for cat in sorted(categories):
                        cat_rules = registry.get_all(category=cat)
                        st.markdown(f"**{cat}** ({len(cat_rules)})")
                        for rule in cat_rules:
                            st.checkbox(
                                f"{rule.rule_id}",
                                value=True,
                                key=f"rule_{rule.rule_id}",
                                help=rule.description
                            )
                else:
                    st.info("æœªåŠ è½½ä»»ä½•æ’ä»¶è§„åˆ™")
            except Exception as e:
                st.warning(f"è§„åˆ™æ’ä»¶ç³»ç»Ÿä¸å¯ç”¨: {e}")
        
        # å…¶ä»–å·¥å…·å¼€å…³
        with st.expander("ğŸ› ï¸ å·¥å…·", expanded=False):
            enable_ruff = st.checkbox("å¯ç”¨ Ruff", value=True, help="å¿«é€Ÿ Python Linter")
            enable_bandit = st.checkbox("å¯ç”¨ Bandit", value=True, help="å®‰å…¨æ¼æ´æ‰«æ")
            enable_radon = st.checkbox("å¯ç”¨ Radon", value=True, help="ä»£ç å¤æ‚åº¦åˆ†æ")
            enable_mypy = st.checkbox("å¯ç”¨ MyPy", value=False, help="é™æ€ç±»å‹æ£€æŸ¥")
        
        # æ—¥å¿—é…ç½®
        with st.expander("ğŸ“‹ æ—¥å¿—è®¾ç½®", expanded=False):
            log_level = st.selectbox("æ—¥å¿—çº§åˆ«", ["DEBUG", "INFO", "WARNING", "ERROR"], index=1)
            use_color_logs = st.checkbox("å½©è‰²è¾“å‡º", value=True)
            log_to_file = st.checkbox("ä¿å­˜åˆ°æ–‡ä»¶", value=False)
            if log_to_file:
                log_file_path = st.text_input("æ—¥å¿—æ–‡ä»¶", value="logs/app.log")
        
        st.markdown("---")
        st.markdown("### ğŸ’¡ å¿«é€ŸæŒ‡å—")
        st.caption("**å·¥ä½œæµç¨‹ï¼š**")
        st.markdown("""
        1. ğŸ” **ä»£ç å®¡æŸ¥** - æ£€æµ‹é—®é¢˜
        2. ğŸ§ª **æµ‹è¯•ç”Ÿæˆ** - åˆ›å»ºå•å…ƒæµ‹è¯•  
        3. ğŸ“„ **æŠ¥å‘Šç”Ÿæˆ** - å¯¼å‡ºç»“æœ
        """)
        st.caption("ğŸ’» åœ¨ä¸»é¢æ¿é€‰æ‹©ä»“åº“å¹¶ç‚¹å‡»ã€Œå…¨éƒ¨è¿è¡Œã€å³å¯å¼€å§‹ã€‚")

    # åŠ è½½å’ŒéªŒè¯é…ç½®
    cfg = load_config(cfg_path, validate=False)
    
    # æ˜¾ç¤ºé…ç½®éªŒè¯çŠ¶æ€
    try:
        validated_cfg = validate_config(cfg)
        st.sidebar.success("âœ… é…ç½®éªŒè¯æˆåŠŸ")
    except ValueError as e:
        st.sidebar.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {str(e)[:100]}")
        st.stop()
    llm_cfg = cfg.get("llm", {})
    with st.sidebar:
        st.markdown("---")
        st.markdown("### LLM Settings")
        llm_enabled = st.checkbox("Enable LLM actions", value=bool(llm_cfg.get("enabled", True)))
        llm_model = st.text_input("Model", value=str(llm_cfg.get("model", "gpt-4o-mini")))
        llm_base_url = st.text_input(
            "Base URL",
            value=str(llm_cfg.get("base_url", "https://api.openai.com/v1")),
        )
        llm_api_key_env = st.text_input(
            "API key env",
            value=str(llm_cfg.get("api_key_env", "OPENAI_API_KEY")),
        )
        llm_api_key = st.text_input("API key (optional)", type="password", value="")
        llm_temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=float(llm_cfg.get("temperature", 0.2)),
            step=0.1,
        )
        llm_max_tokens = st.number_input(
            "Max tokens",
            min_value=256,
            max_value=8192,
            value=int(llm_cfg.get("max_tokens", 1200)),
            step=128,
        )

    cfg.setdefault("llm", {})
    cfg["llm"]["enabled"] = llm_enabled
    cfg["llm"]["model"] = llm_model
    cfg["llm"]["base_url"] = llm_base_url
    cfg["llm"]["api_key_env"] = llm_api_key_env
    cfg["llm"]["temperature"] = llm_temperature
    cfg["llm"]["max_tokens"] = llm_max_tokens
    if llm_api_key:
        cfg["llm"]["api_key"] = llm_api_key
    # åº”ç”¨ GUI è®¾ç½®åˆ°é…ç½®
    cfg.setdefault("review", {})
    cfg["review"]["enable_ds_rules"] = enable_ds_basic
    cfg["review"]["enable_ds_rules_advanced"] = enable_ds_advanced
    cfg["review"]["enable_ruff"] = enable_ruff
    cfg["review"]["enable_bandit"] = enable_bandit
    cfg["review"]["enable_radon"] = enable_radon
    cfg["review"]["enable_mypy"] = enable_mypy
    
    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    if 'logger_configured' not in st.session_state:
        import logging
        level_map = {"DEBUG": logging.DEBUG, "INFO": logging.INFO, 
                     "WARNING": logging.WARNING, "ERROR": logging.ERROR}
        setup_logger(
            "code-assistant",
            level=level_map[log_level],
            use_color=use_color_logs,
            log_file=log_file_path if log_to_file else None
        )
        st.session_state['logger_configured'] = True
    
    st.markdown("---")
    st.markdown("### ğŸš€ æ‰§è¡Œæ“ä½œ")
    col_a, col_b, col_c = st.columns(3, gap="medium")
    
    with col_a:
        st.markdown("**ğŸ” ä»£ç å®¡æŸ¥**")
        st.caption("é™æ€åˆ†æã€å®‰å…¨æ£€æŸ¥å’Œæ•°æ®ç§‘å­¦è§„åˆ™")
        run_review = st.button("å¼€å§‹å®¡æŸ¥", use_container_width=True)
    with col_b:
        st.markdown("**ğŸ§ª æµ‹è¯•ç”Ÿæˆ**")
        st.caption("ç”Ÿæˆå•å…ƒæµ‹è¯•å’Œè¦†ç›–ç‡æŠ¥å‘Š")
        run_testgen = st.button("ç”Ÿæˆæµ‹è¯•", use_container_width=True)
    with col_c:
        st.markdown("**âš¡ å®Œæ•´æµç¨‹**")
        st.caption("å®¡æŸ¥ + æµ‹è¯• + æŠ¥å‘Šä¸€æ­¥å®Œæˆ")
        run_all = st.button("å…¨éƒ¨è¿è¡Œ", type="primary", use_container_width=True, help="ä¸€é”®æ‰§è¡Œå®¡æŸ¥ã€æµ‹è¯•ç”Ÿæˆå’ŒæŠ¥å‘Šç”Ÿæˆ")

    st.divider()

    state = st.session_state
    state.setdefault("last_review", None)
    state.setdefault("last_testgen", None)
    state.setdefault("last_report_path", None)
    state.setdefault("last_report_pdf", None)
    state.setdefault("batch_results", [])
    state.setdefault("last_repo_path", None)
    state.setdefault("llm_plan", None)
    state.setdefault("llm_plan_source", None)
    state.setdefault("llm_changes", None)
    state.setdefault("llm_recommendations", None)

    repo_jobs: List[Dict[str, str]] = []
    if run_review or run_testgen or run_all:
        repo_inputs = _parse_repo_inputs(repo_input)
        repo_jobs = _resolve_repo_inputs(repo_input, cache_dir)
        if not repo_jobs:
            st.stop()
        if len(repo_jobs) > 1:
            st.info(f"æ‰¹é‡æ¨¡å¼ï¼š{len(repo_jobs)} ä¸ªä»“åº“ã€‚")
        elif repo_inputs and repo_jobs[0]["path"] != repo_inputs[0]:
            st.caption(f"å·²è§£æä»“åº“ï¼š{repo_jobs[0]['path']}")

        state["batch_results"] = []
        batch_mode = len(repo_jobs) > 1
        if batch_mode:
            state["last_review"] = None
            state["last_testgen"] = None
            state["last_report_path"] = None
            state["last_report_pdf"] = None

        for job in repo_jobs:
            repo_out = Path(out_dir) / job["name"] if batch_mode else Path(out_dir)
            _ensure_dirs(str(repo_out))

            test_out = None
            if batch_mode and (run_testgen or run_all):
                test_out = repo_out / "generated_tests"

            cfg_run = _prepare_cfg(cfg, test_out)
            orch = Orchestrator(cfg_run)

            review = None
            if run_review or run_all:
                st.info(f"ğŸ“Š æ­£åœ¨æ‰«æ {job['name']} çš„é—®é¢˜...ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰")
                with st.spinner(f"æ­£åœ¨æ‰§è¡Œå®¡æŸ¥ï¼š{job['name']}"):
                    review = orch.run_review(repo_path=job["path"])
                (repo_out / "review.json").write_text(
                    json.dumps(review, ensure_ascii=False, indent=2),
                    encoding="utf-8",
                )
                st.success(f"âœ… [{job['name']}] å®¡æŸ¥å®Œæˆï¼šå‘ç° {len(review['findings'])} ä¸ªé—®é¢˜ã€‚")

            testgen = None
            if run_testgen or run_all:
                st.info(f"ğŸ§ª ä¸º {job['name']} ç”Ÿæˆæµ‹è¯•...")
                with st.spinner(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•ï¼š{job['name']}"):
                    testgen = orch.run_testgen(repo_path=job["path"])
                (repo_out / "testgen.json").write_text(
                    json.dumps(testgen, ensure_ascii=False, indent=2),
                    encoding="utf-8",
                )
                st.success(
                    f"âœ… [{job['name']}] æµ‹è¯•ç”Ÿæˆå®Œæˆï¼š{testgen['written_files']} ä¸ªæ–‡ä»¶å·²å†™å…¥"
                    f"{testgen['output_dir']}ã€‚"
                )
                st.success(
                    f"[{job['name']}] Test generation complete: {testgen['written_files']} files written to "
                    f"{testgen['output_dir']}."
                )

            report_path = None
            pdf_path = None
            if run_all:
                st.info(f"ğŸ“„ ä¸º {job['name']} ç”ŸæˆæŠ¥å‘Š...")
                with st.spinner(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Šï¼š{job['name']}"):
                    report_path, pdf_path = _write_report(str(repo_out), review, testgen)
                st.success(f"âœ… [{job['name']}] æŠ¥å‘Šå·²ä¿å­˜ï¼š{report_path}")

            if not batch_mode:
                state['last_repo_path'] = job['path']
                if review is not None:
                    state["last_review"] = review
                if testgen is not None:
                    state["last_testgen"] = testgen
                if report_path:
                    state["last_report_path"] = str(report_path)
                    state["last_report_pdf"] = str(pdf_path) if pdf_path else None

            state["batch_results"].append(
                {
                    "name": job["name"],
                    "path": job["path"],
                    "out_dir": str(repo_out),
                    "review_count": len(review["findings"]) if review else None,
                    "testgen_written": testgen.get("written_files") if testgen else None,
                    "report_path": str(report_path) if report_path else None,
                    "pdf_path": str(pdf_path) if pdf_path else None,
                }
            )

    batch_results = state.get("batch_results") or []
    batch_mode = len(batch_results) > 1
    active_out_dir = out_dir
    active_review = state.get("last_review")
    active_testgen = state.get("last_testgen")
    active_item = None

    if batch_mode:
        st.markdown("---")
        st.markdown("### ğŸ“¦ æ‰¹é‡å¤„ç†ç»“æœ")
        st.caption("å¤šä¸ªä»“åº“çš„åˆ†æç»“æœæ±‡æ€»")
        
        # ä½¿ç”¨è¡¨æ ¼å½¢å¼å±•ç¤ºç»“æœ
        for item in batch_results:
            review_part = (
                f"âœ“ {item['review_count']} ä¸ªé—®é¢˜" if item.get("review_count") is not None else "âŠ˜ æœªå®¡æŸ¥"
            )
            test_part = (
                f"âœ“ {item['testgen_written']} ä¸ªæµ‹è¯•" if item.get("testgen_written") is not None else "âŠ˜ æœªç”Ÿæˆ"
            )
            report_part = "âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ" if item.get("report_path") else "âŠ˜ æ— æŠ¥å‘Š"
            
            st.markdown(f"**ğŸ“ {item['name']}** Â· {review_part} Â· {test_part} Â· {report_part}")
        
        st.markdown("---")
        st.markdown("#### ğŸ” é€‰æ‹©ä»“åº“æŸ¥çœ‹è¯¦æƒ…")
        labels = [item["name"] for item in batch_results]
        active_name = st.selectbox(
            "é€‰æ‹©è¦æ£€æŸ¥çš„ä»“åº“",
            labels,
            key="batch_select",
            help="é€‰æ‹©ä¸€ä¸ªä»“åº“ä»¥æŸ¥çœ‹å…¶è¯¦ç»†çš„å®¡æŸ¥ç»“æœå’Œæµ‹è¯•æŠ¥å‘Š"
        )
        active_item = next((i for i in batch_results if i["name"] == active_name), None)
        if active_item:
            active_out_dir = active_item["out_dir"]
            active_review, active_testgen = _load_report_sources(active_out_dir, {})

    review = active_review
    if review:
        title = "### ğŸ“‹ å®¡æŸ¥æŠ¥å‘Š"
        if batch_mode and active_item:
            title = f"### ğŸ“‹ å®¡æŸ¥æŠ¥å‘Šï¼ˆ{active_item['name']}ï¼‰"
        findings = review["findings"]
        st.markdown(title)
        
        if len(findings) == 0:
            st.success("ğŸ‰ æœªå‘ç°é—®é¢˜ï¼ä»£ç è´¨é‡ä¼˜ç§€ï¼ˆæˆ–æ‰«æå™¨å·²ç¦ç”¨ï¼‰ã€‚")
        else:
            # åˆ†ç±»ç»Ÿè®¡
            ds_rules = [f for f in findings if f.get("tool") in ["ds-rule", "ds-rule-advanced"]]
            plugin_rules = [f for f in findings if f.get("tool") == "rule-plugin"]
            other_findings = [f for f in findings if f.get("tool") not in ["ds-rule", "ds-rule-advanced", "rule-plugin"]]
            
            # ä¸»è¦æŒ‡æ ‡ - ä½¿ç”¨å¡ç‰‡å¼å¸ƒå±€
            st.markdown("#### ğŸ“Š é—®é¢˜æ¦‚è§ˆ")
            metric_cols = st.columns(4, gap="medium")
            metric_cols[0].metric("ğŸ” å‘ç°é—®é¢˜æ€»æ•°", len(findings))
            metric_cols[1].metric("ğŸ¯ DS è§„åˆ™", len(ds_rules))
            metric_cols[2].metric("ğŸ”Œ æ’ä»¶è§„åˆ™", len(plugin_rules))
            metric_cols[3].metric("ğŸ› ï¸ å…¶ä»–å·¥å…·", len(other_findings))
            
            st.markdown("")  # æ·»åŠ é—´è·

            # å±•å¼€æ–°è§„åˆ™çš„è¯¦ç»†ä¿¡æ¯
            if ds_rules:
                with st.expander("ğŸ¯ æ•°æ®ç§‘å­¦è§„åˆ™è¯¦æƒ…", expanded=True):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**è§„åˆ™åˆ†å¸ƒ**")
                        ds_rule_names = [f.get("rule") for f in ds_rules]
                        _plot_counts(ds_rule_names, "DS è§„åˆ™ç±»å‹")
                    
                    with col2:
                        st.markdown("**ä¸¥é‡æ€§åˆ†å¸ƒ**")
                        ds_severities = [f.get("severity") for f in ds_rules]
                        _plot_counts(ds_severities, "DS ä¸¥é‡æ€§")
                    
                    # æ˜¾ç¤ºå…³é”® DS é—®é¢˜
                    st.markdown("**å…³é”®é—®é¢˜**")
                    ds_high = [f for f in ds_rules if f.get("severity") == "high"]
                    if ds_high:
                        for f in ds_high[:5]:
                            st.warning(f"âš ï¸ {f.get('rule')}ï¼š{f.get('message')}ï¼ˆç¬¬ {f.get('line', 'N/A')} è¡Œï¼‰")
                    else:
                        st.success("âœ… æœªå‘ç°é«˜å± DS é—®é¢˜")
            
            if plugin_rules:
                with st.expander("ğŸ”Œ æ’ä»¶è§„åˆ™è¯¦æƒ…", expanded=True):
                    plugin_rule_names = [f.get("rule") for f in plugin_rules]
                    _plot_counts(plugin_rule_names, "æ’ä»¶è§„åˆ™ç±»å‹")
                    
                    # æŒ‰ç±»åˆ«åˆ†ç»„
                    plugin_by_rule = {}
                    for f in plugin_rules:
                        rule = f.get("rule", "unknown")
                        plugin_by_rule.setdefault(rule, []).append(f)
                    
                    for rule, items in plugin_by_rule.items():
                        st.markdown(f"**{rule}** ({len(items)} ä¸ª)")
                        for item in items[:3]:
                            st.caption(f"- {item.get('file')}:{item.get('line')} - {item.get('message')}")

            # æ€»è§ˆå›¾è¡¨ - ä½¿ç”¨åˆ†éš”çº¿å’Œæ›´å¥½çš„å¸ƒå±€
            st.markdown("---")
            st.markdown("#### ğŸ“Š æ•°æ®å¯è§†åŒ–")
            chart_cols = st.columns(3, gap="large")
            
            with chart_cols[0]:
                st.markdown("**ğŸ¨ ä¸¥é‡æ€§åˆ†å¸ƒ**")
                sev = [f["severity"] for f in findings]
                _plot_counts(sev, "")
            
            with chart_cols[1]:
                st.markdown("**ğŸ”§ å·¥å…·åˆ†å¸ƒ**")
                tools = [f["tool"] for f in findings]
                _plot_counts(tools, "")
            
            with chart_cols[2]:
                st.markdown("**ğŸ“ é—®é¢˜æ–‡ä»¶ Top 10**")
                # æ–‡ä»¶åˆ†å¸ƒ
                files = [f.get("file", "unknown") for f in findings]
                file_counts = Counter(files).most_common(10)
                _plot_counts(file_counts, "")

            # è¯¦ç»†é—®é¢˜åˆ—è¡¨
            st.markdown("---")
            st.markdown("#### ğŸ” è¯¦ç»†é—®é¢˜åˆ—è¡¨ï¼ˆTop 20ï¼‰")
            st.caption("æŒ‰ä¸¥é‡æ€§å’Œå·¥å…·æ’åºçš„å‰ 20 ä¸ªé—®é¢˜")
            _show_findings_table(findings[:20])

    # æµ‹è¯•ç”Ÿæˆç»“æœ
    testgen = active_testgen
    if testgen:
        st.markdown("---")
        title = "### ğŸ§ª æµ‹è¯•ç”Ÿæˆç»“æœ"
        if batch_mode and active_item:
            title = f"### ğŸ§ª æµ‹è¯•ç”Ÿæˆç»“æœï¼ˆ{active_item['name']}ï¼‰"
        st.markdown(title)
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°", testgen.get("written_files", 0))
        col2.metric("ğŸ“‚ è¾“å‡ºç›®å½•", testgen.get("output_dir", "N/A"))
        col3.metric("âœ… çŠ¶æ€", "å®Œæˆ" if testgen.get("written_files", 0) > 0 else "æ— ")
        out_dir_val = str(testgen.get("output_dir", "N/A"))
        copy_key = f"testgen_outdir_copy_{active_item['name']}" if batch_mode and active_item else "testgen_outdir_copy"
        st.text_input("Output dir (copyable)", value=out_dir_val, key=copy_key)
        
        # è¯¦ç»†ä¿¡æ¯ï¼ˆæŠ˜å ï¼‰
        with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯", expanded=False):
            st.json({k: v for k, v in testgen.items() if k != "generated"})

    # æŠ¥å‘Šä¸‹è½½å’Œé¢„è§ˆ
    st.markdown("---")
    report_title = "### Report"
    if batch_mode and active_item:
        report_title = f"### Report ({active_item['name']})"
    st.markdown(report_title)

    rp = None
    if batch_mode and active_item:
        candidate = Path(active_out_dir) / "report.md"
        if candidate.exists():
            rp = candidate
    else:
        rp = _existing_report_path(out_dir, state)

    if not rp:
        if active_review or active_testgen:
            button_label = "Build report for selected repo" if batch_mode else "Build report from latest run"
            if st.button(button_label, key="build_report"):
                report_path, pdf_path = _write_report(active_out_dir, active_review, active_testgen)
                if batch_mode and active_item:
                    active_item["report_path"] = str(report_path)
                    active_item["pdf_path"] = str(pdf_path) if pdf_path else None
                else:
                    state["last_report_path"] = str(report_path)
                    state["last_report_pdf"] = str(pdf_path) if pdf_path else None
                rp = report_path
        else:
            st.info("Run Review/TestGen/All to generate a report.")

    if rp and rp.exists():
        source_state = {} if batch_mode else state
        review_src, testgen_src = _load_report_sources(active_out_dir, source_state)
        if batch_mode and active_item:
            pdf_path = active_item.get("pdf_path")
        else:
            pdf_path = state.get("last_report_pdf")
        pdf_bytes = None
        if pdf_path and Path(pdf_path).exists():
            pdf_bytes = Path(pdf_path).read_bytes()
        else:
            md_text = rp.read_text(encoding="utf-8")
            tex_path = rp.with_suffix(".tex")
            if not tex_path.exists():
                tex_path.write_text(build_latex_report(review_src, testgen_src), encoding="utf-8")
            compiled = _compile_latex(tex_path)
            if compiled:
                pdf_path = compiled
                pdf_bytes = compiled.read_bytes()
                if batch_mode and active_item:
                    active_item["pdf_path"] = str(pdf_path)
                else:
                    state["last_report_pdf"] = str(pdf_path)
            else:
                pdf_bytes = _make_pdf_bytes(review_src, testgen_src, md_text)
                if pdf_bytes:
                    pdf_path = rp.with_suffix(".pdf")
                    pdf_path.write_bytes(pdf_bytes)
                    if batch_mode and active_item:
                        active_item["pdf_path"] = str(pdf_path)
                    else:
                        state["last_report_pdf"] = str(pdf_path)
        if pdf_bytes:
            btn_col, info_col = st.columns([1, 3], gap="large")
            file_prefix = f"{active_item['name']}_" if batch_mode and active_item else ""
            with btn_col:
                st.download_button(
                    "Download report.pdf",
                    data=pdf_bytes,
                    file_name=f"{file_prefix}report.pdf",
                    mime="application/pdf",
                )
                st.download_button(
                    "Download report.md",
                    data=rp.read_bytes(),
                    file_name=f"{file_prefix}report.md",
                )
            with info_col:
                st.caption("PDF preview")
            _render_pdf_preview(pdf_bytes)
        else:
            st.info("PDF preview unavailable. Install reportlab to enable PDF export.")

        st.markdown("---")
        st.markdown("### LLM Actions")
        llm_cfg = cfg.get("llm", {})
        if not llm_cfg.get("enabled", True):
            st.info("LLM actions are disabled. Enable them in the LLM settings.")
        else:
            ready, reason = _llm_ready(cfg)
            if not ready:
                st.warning(f"LLM not configured: {reason}")
            else:
                repo_root = None
                if review_src and review_src.get("repo"):
                    repo_root = Path(review_src.get("repo"))
                elif testgen_src and testgen_src.get("repo"):
                    repo_root = Path(testgen_src.get("repo"))
                elif batch_mode and active_item:
                    repo_root = Path(active_item.get("path", ""))
                elif state.get("last_repo_path"):
                    repo_root = Path(state.get("last_repo_path"))

                if not repo_root or not repo_root.exists():
                    st.warning("Repo path not found for LLM actions.")
                else:
                    repo_key = str(repo_root)
                    meta = state.get("llm_plan_source") or {}
                    if isinstance(meta, dict) and meta.get("repo") != repo_key:
                        state["llm_plan"] = None
                        state["llm_changes"] = None

                    report_text = _truncate_text(build_markdown_report(review_src, testgen_src), 12000)
                    files_ctx = _collect_context_files(repo_root, review_src)
                    ctx_text = _format_file_context(files_ctx) if files_ctx else ""
                    if not ctx_text:
                        st.caption("No file context extracted from the report; LLM output may be limited.")

                    plan_col, rec_col = st.columns(2, gap="medium")
                    with plan_col:
                        if st.button("Generate fix plan", key="llm_plan_btn"):
                            plan_res = _llm_generate_plan(report_text, ctx_text, cfg)
                            if plan_res.get("ok"):
                                state["llm_plan"] = plan_res.get("plan")
                                state["llm_plan_source"] = {"repo": repo_key, "raw": plan_res.get("raw", "")}
                                state["llm_changes"] = None
                                st.success("Plan generated.")
                            else:
                                st.error(plan_res.get("error") or "LLM plan failed.")
                    with rec_col:
                        if st.button("Get recommendations", key="llm_rec_btn"):
                            rec_res = _llm_generate_recommendations(report_text, cfg)
                            if rec_res.get("ok"):
                                state["llm_recommendations"] = rec_res.get("projects")
                                st.success("Recommendations ready.")
                            else:
                                st.error(rec_res.get("error") or "LLM recommendations failed.")

                    plan = state.get("llm_plan") or []
                    if plan:
                        st.markdown("#### Proposed changes")
                        for item in plan:
                            file_part = f" ({item.get('file')})" if item.get("file") else ""
                            st.markdown(f"- {item.get('summary')}{file_part}")

                        if st.button("Apply changes", key="llm_apply_btn"):
                            apply_res = _llm_generate_changes(report_text, plan, ctx_text, cfg)
                            if apply_res.get("ok"):
                                changed, skipped = _apply_llm_changes(
                                    repo_root,
                                    apply_res.get("files") or [],
                                    allow_new=bool(llm_cfg.get("allow_new_files", False)),
                                )
                                state["llm_changes"] = {
                                    "changed": [str(p) for p in changed],
                                    "skipped": skipped,
                                }
                                if changed:
                                    zip_bytes = _build_changes_zip(changed, repo_root)
                                    st.download_button(
                                        "Download modified files",
                                        data=zip_bytes,
                                        file_name="llm_changes.zip",
                                    )
                                    st.success(f"Applied changes to {len(changed)} files.")
                                if skipped:
                                    st.warning(f"Skipped {len(skipped)} file entries.")
                            else:
                                st.error(apply_res.get("error") or "LLM apply failed.")

                    recs = state.get("llm_recommendations") or []
                    if recs:
                        st.markdown("#### Recommendations")
                        for rec in recs:
                            name = rec.get("name") or "(no name)"
                            url = rec.get("url") or ""
                            why = rec.get("why") or ""
                            if url:
                                st.markdown(f"- [{name}]({url}) - {why}")
                            else:
                                st.markdown(f"- {name} - {why}")


if __name__ == "__main__":
    main()

























