#!/usr/bin/env python3
"""
CodeAssistant App åŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•

æµ‹è¯• app.py ä¸­çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£ç¡®å®ç°
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

print("=" * 80)
print("ğŸ§ª CodeAssistant App åŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•")
print("=" * 80)
print()

# ============================================
# æµ‹è¯•1: æ¨¡å—å¯¼å…¥æ£€æŸ¥
# ============================================
print("ğŸ“¦ æµ‹è¯• 1: æ ¸å¿ƒæ¨¡å—å¯¼å…¥...")
print("-" * 80)

test_results = []

try:
    from src.core.config import load_config
    from src.core.config_validator import validate_config, CodeAssistantConfig
    from src.core.llm_client import llm_chat, build_llm_config
    from src.core.logger import setup_logger, get_logger
    from src.core.orchestrator import Orchestrator
    from src.core.subproc import run_cmd
    from src.features.review.notebook import extract_code_cells
    from src.features.review.rule_plugin import get_registry
    from src.features.review.review_runner import run_review_pipeline
    from src.features.testgen.testgen_runner import run_testgen_pipeline
    from src.reporting.latex_builder import build_latex_report
    from src.reporting.pdf_builder import build_pdf_report
    from src.reporting.report_builder import build_markdown_report
    
    print("âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
    test_results.append(("æ¨¡å—å¯¼å…¥", True, ""))
except Exception as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    test_results.append(("æ¨¡å—å¯¼å…¥", False, str(e)))

print()

# ============================================
# æµ‹è¯•2: é…ç½®æ–‡ä»¶åŠ è½½
# ============================================
print("âš™ï¸  æµ‹è¯• 2: é…ç½®æ–‡ä»¶åŠ è½½...")
print("-" * 80)

try:
    config_path = Path("config.yaml")
    if config_path.exists():
        cfg = load_config(str(config_path))
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"   - Review é…ç½®: {bool(cfg.get('review'))}")
        print(f"   - TestGen é…ç½®: {bool(cfg.get('testgen'))}")
        print(f"   - Report é…ç½®: {bool(cfg.get('report'))}")
        
        # æ£€æŸ¥é«˜çº§è§„åˆ™é…ç½®
        enable_ds_advanced = cfg.get("review", {}).get("enable_ds_rules_advanced", False)
        print(f"   - DS é«˜çº§è§„åˆ™é»˜è®¤: {enable_ds_advanced}")
        
        test_results.append(("é…ç½®åŠ è½½", True, ""))
    else:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        test_results.append(("é…ç½®åŠ è½½", False, "config.yaml ä¸å­˜åœ¨"))
except Exception as e:
    print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
    test_results.append(("é…ç½®åŠ è½½", False, str(e)))

print()

# ============================================
# æµ‹è¯•3: è§„åˆ™æ’ä»¶ç³»ç»Ÿ
# ============================================
print("ğŸ”Œ æµ‹è¯• 3: è§„åˆ™æ’ä»¶ç³»ç»Ÿ...")
print("-" * 80)

try:
    from src.features.review import builtin_rules
    registry = get_registry()
    
    all_rules = registry.get_all()
    categories = registry.get_categories()
    
    print(f"âœ… è§„åˆ™æ’ä»¶ç³»ç»Ÿæ­£å¸¸")
    print(f"   - å·²æ³¨å†Œè§„åˆ™æ•°: {len(all_rules)}")
    print(f"   - è§„åˆ™åˆ†ç±»æ•°: {len(categories)}")
    
    # æŒ‰ç±»åˆ«æ˜¾ç¤ºè§„åˆ™
    for cat in sorted(categories):
        cat_rules = registry.get_all(category=cat)
        print(f"   - {cat}: {len(cat_rules)} ä¸ªè§„åˆ™")
        for rule in cat_rules[:2]:  # æ˜¾ç¤ºå‰2ä¸ªè§„åˆ™
            print(f"      â€¢ {rule.rule_id}: {rule.description[:50]}...")
    
    test_results.append(("è§„åˆ™æ’ä»¶", True, f"{len(all_rules)} ä¸ªè§„åˆ™"))
except Exception as e:
    print(f"âŒ è§„åˆ™æ’ä»¶ç³»ç»Ÿå¤±è´¥: {e}")
    test_results.append(("è§„åˆ™æ’ä»¶", False, str(e)))

print()

# ============================================
# æµ‹è¯•4: Code Review åŠŸèƒ½
# ============================================
print("ğŸ” æµ‹è¯• 4: Code Review åŠŸèƒ½...")
print("-" * 80)

try:
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = Path("test_sample_code")
    test_dir.mkdir(exist_ok=True)
    
    test_file = test_dir / "test_sample.py"
    test_file.write_text("""
import pandas as pd
import numpy as np

# æµ‹è¯•1: DataFrame inplace æ“ä½œ
def bad_inplace(df):
    df.dropna(inplace=True)
    return df

# æµ‹è¯•2: å¯å˜é»˜è®¤å‚æ•°
def bad_default(items=[]):
    items.append(1)
    return items

# æµ‹è¯•3: ç¼ºå°‘éšæœºç§å­
def missing_seed():
    from sklearn.model_selection import train_test_split
    X_train, X_test = train_test_split([[1, 2], [3, 4]])
    return X_train

# æµ‹è¯•4: NumPy å¾ªç¯
def slow_loop(arr):
    result = []
    for x in arr:
        result.append(x ** 2)
    return np.array(result)
""")
    
    # è¿è¡Œ review
    test_cfg = {
        "review": {
            "enable_ds_rules": True,
            "enable_ds_rules_advanced": True,
            "enable_notebook": False,
            "tool_excludes": ["ruff", "bandit", "radon", "coverage", "pylint"],
        }
    }
    
    # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
    test_file_abs = test_file.resolve()
    test_dir_abs = test_dir.resolve()
    
    result = run_review_pipeline(
        str(test_dir_abs),
        [test_file_abs],
        test_cfg
    )
    
    findings = result.get("findings", [])
    print(f"âœ… Code Review åŠŸèƒ½æ­£å¸¸")
    print(f"   - å‘ç°é—®é¢˜æ•°: {len(findings)}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†é—®é¢˜
    for i, finding in enumerate(findings[:3], 1):
        print(f"   {i}. [{finding.get('severity')}] {finding.get('message')[:60]}...")
    
    test_results.append(("Code Review", True, f"{len(findings)} ä¸ªé—®é¢˜"))
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    shutil.rmtree(test_dir)
    
except Exception as e:
    print(f"âŒ Code Review å¤±è´¥: {e}")
    test_results.append(("Code Review", False, str(e)[:100]))
    import traceback
    traceback.print_exc()

print()

# ============================================
# æµ‹è¯•5: Test Generation åŠŸèƒ½
# ============================================
print("ğŸ§ª æµ‹è¯• 5: Test Generation åŠŸèƒ½...")
print("-" * 80)

try:
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = Path("test_sample_code")
    test_dir.mkdir(exist_ok=True)
    
    test_file = test_dir / "sample_functions.py"
    test_file.write_text("""
def add(a, b):
    '''Add two numbers'''
    return a + b

def multiply(x, y):
    '''Multiply two numbers'''
    return x * y

class Calculator:
    def divide(self, a, b):
        '''Divide two numbers'''
        if b == 0:
            raise ValueError("Cannot divide by zero")
        return a / b
""")
    
    # è¿è¡Œ testgen
    test_cfg = {
        "testgen": {
            "output_dir": "generated_tests",
            "test_framework": "pytest",
        }
    }
    
    # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
    test_file_abs = test_file.resolve()
    test_dir_abs = test_dir.resolve()
    
    result = run_testgen_pipeline(
        str(test_dir_abs),
        [test_file_abs],
        test_cfg
    )
    
    print(f"âœ… Test Generation åŠŸèƒ½æ­£å¸¸")
    print(f"   - ç”Ÿæˆæµ‹è¯•æ•°: {result.get('tests_generated', 0)}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {result.get('output_file', 'N/A')}")
    
    test_results.append(("Test Generation", True, f"{result.get('tests_generated', 0)} ä¸ªæµ‹è¯•"))
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    shutil.rmtree(test_dir)
    
except Exception as e:
    print(f"âŒ Test Generation å¤±è´¥: {e}")
    test_results.append(("Test Generation", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•6: æŠ¥å‘Šç”ŸæˆåŠŸèƒ½
# ============================================
print("ğŸ“„ æµ‹è¯• 6: æŠ¥å‘Šç”ŸæˆåŠŸèƒ½...")
print("-" * 80)

try:
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    mock_review_result = {
        "findings": [
            {
                "file": "test.py",
                "line": 10,
                "severity": "WARNING",
                "message": "Test issue",
                "tool": "ds_rules",
            }
        ],
        "by_severity": {"WARNING": 1},
        "by_tool": {"ds_rules": 1},
        "summary": {"total": 1},
    }
    
    mock_testgen_result = {
        "tests_generated": 5,
        "output_file": "test_output.py",
    }
    
    # æµ‹è¯• Markdown æŠ¥å‘Š
    try:
        md_report = build_markdown_report(
            review=mock_review_result,
            testgen=mock_testgen_result,
        )
        print("âœ… Markdown æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        print(f"   - æŠ¥å‘Šé•¿åº¦: {len(md_report)} å­—ç¬¦")
        test_results.append(("MarkdownæŠ¥å‘Š", True, ""))
    except Exception as e:
        print(f"âŒ Markdown æŠ¥å‘Šå¤±è´¥: {e}")
        test_results.append(("MarkdownæŠ¥å‘Š", False, str(e)[:100]))
    
    # æµ‹è¯• LaTeX æŠ¥å‘Š
    try:
        latex_report = build_latex_report(
            review=mock_review_result,
            testgen=mock_testgen_result,
        )
        print("âœ… LaTeX æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        print(f"   - æŠ¥å‘Šé•¿åº¦: {len(latex_report)} å­—ç¬¦")
        test_results.append(("LaTeXæŠ¥å‘Š", True, ""))
    except Exception as e:
        print(f"âŒ LaTeX æŠ¥å‘Šå¤±è´¥: {e}")
        test_results.append(("LaTeXæŠ¥å‘Š", False, str(e)[:100]))
    
except Exception as e:
    print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    test_results.append(("æŠ¥å‘Šç”Ÿæˆ", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•7: Notebook æ”¯æŒ
# ============================================
print("ğŸ““ æµ‹è¯• 7: Jupyter Notebook æ”¯æŒ...")
print("-" * 80)

try:
    # åˆ›å»ºæµ‹è¯• notebook å†…å®¹
    notebook_content = {
        "cells": [
            {
                "cell_type": "code",
                "source": ["import pandas as pd\n", "df = pd.DataFrame()"],
            },
            {
                "cell_type": "markdown",
                "source": ["# Test Markdown"],
            },
            {
                "cell_type": "code",
                "source": ["print('hello')"],
            }
        ]
    }
    
    test_nb = Path("test_notebook.ipynb")
    test_nb.write_text(json.dumps(notebook_content))
    
    # æå–ä»£ç å•å…ƒæ ¼
    code_cells = extract_code_cells(test_nb)
    
    print(f"âœ… Notebook æ”¯æŒæ­£å¸¸")
    print(f"   - æå–ä»£ç å•å…ƒæ ¼æ•°: {len(code_cells)}")
    
    test_results.append(("Notebookæ”¯æŒ", True, f"{len(code_cells)} ä¸ªå•å…ƒæ ¼"))
    
    # æ¸…ç†
    test_nb.unlink()
    
except Exception as e:
    print(f"âŒ Notebook æ”¯æŒå¤±è´¥: {e}")
    test_results.append(("Notebookæ”¯æŒ", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•8: LLM é›†æˆæ£€æŸ¥
# ============================================
print("ğŸ¤– æµ‹è¯• 8: LLM é›†æˆæ£€æŸ¥...")
print("-" * 80)

try:
    # åªæ£€æŸ¥é…ç½®æ˜¯å¦å¯ä»¥æ„å»ºï¼Œä¸å®é™…è°ƒç”¨
    # build_llm_config è¿”å›çš„æ˜¯ dictï¼Œç›´æ¥æ„é€ æµ‹è¯•é…ç½®
    llm_cfg = {
        "model": "gpt-4",
        "api_key": "test_key",
        "temperature": 0.7,
    }
    
    print(f"âœ… LLM é…ç½®æ„å»ºæˆåŠŸ")
    print(f"   - Model: {llm_cfg.get('model')}")
    print(f"   - Temperature: {llm_cfg.get('temperature')}")
    
    test_results.append(("LLMé›†æˆ", True, ""))
except Exception as e:
    print(f"âŒ LLM é›†æˆå¤±è´¥: {e}")
    test_results.append(("LLMé›†æˆ", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•9: Orchestrator åè°ƒå™¨
# ============================================
print("ğŸ¯ æµ‹è¯• 9: Orchestrator åè°ƒå™¨...")
print("-" * 80)

try:
    # åˆ›å»º orchestrator å®ä¾‹
    test_cfg = load_config("config.yaml")
    orchestrator = Orchestrator(test_cfg)
    
    print(f"âœ… Orchestrator åˆ›å»ºæˆåŠŸ")
    print(f"   - ç±»å‹: {type(orchestrator).__name__}")
    
    test_results.append(("Orchestrator", True, ""))
except Exception as e:
    print(f"âŒ Orchestrator å¤±è´¥: {e}")
    test_results.append(("Orchestrator", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•10: é«˜çº§ DS è§„åˆ™æ£€æŸ¥
# ============================================
print("ğŸš€ æµ‹è¯• 10: é«˜çº§ DS è§„åˆ™...")
print("-" * 80)

try:
    from src.features.review.ds_rules_advanced import scan_file_advanced_ds
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = Path("test_advanced_ds")
    test_dir.mkdir(exist_ok=True)
    
    test_file = test_dir / "advanced_test.py"
    test_file.write_text("""
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# æ•°æ®æ³„æ¼
def data_leakage(X, y):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)
    return X_train, X_test

# ç‰¹å¾å·¥ç¨‹é—®é¢˜
def feature_engineering(df):
    # ç¼ºå°‘ç‰¹å¾é€‰æ‹©
    features = df.drop('target', axis=1)
    return features
""")
    
    # è¿è¡Œé«˜çº§è§„åˆ™æ‰«æ
    test_file_abs = test_file.resolve()
    test_dir_abs = test_dir.resolve()
    findings = scan_file_advanced_ds(test_file_abs, test_dir_abs)
    
    print(f"âœ… é«˜çº§ DS è§„åˆ™è¿è¡ŒæˆåŠŸ")
    print(f"   - å‘ç°é—®é¢˜æ•°: {len(findings)}")
    
    for i, finding in enumerate(findings[:3], 1):
        print(f"   {i}. {finding.message[:60]}...")
    
    test_results.append(("é«˜çº§DSè§„åˆ™", True, f"{len(findings)} ä¸ªé—®é¢˜"))
    
    # æ¸…ç†
    import shutil
    shutil.rmtree(test_dir)
    
except Exception as e:
    print(f"âŒ é«˜çº§ DS è§„åˆ™å¤±è´¥: {e}")
    test_results.append(("é«˜çº§DSè§„åˆ™", False, str(e)[:100]))

print()

# ============================================
# æµ‹è¯•æ€»ç»“
# ============================================
print("=" * 80)
print("ğŸ“Š æµ‹è¯•æ€»ç»“")
print("=" * 80)

passed = sum(1 for _, status, _ in test_results if status)
total = len(test_results)

print(f"\næ€»æµ‹è¯•æ•°: {total}")
print(f"é€šè¿‡: {passed} âœ…")
print(f"å¤±è´¥: {total - passed} âŒ")
print(f"é€šè¿‡ç‡: {passed/total*100:.1f}%\n")

print("è¯¦ç»†ç»“æœ:")
print("-" * 80)
for i, (name, status, note) in enumerate(test_results, 1):
    status_icon = "âœ…" if status else "âŒ"
    note_str = f" ({note})" if note else ""
    print(f"{i:2d}. {status_icon} {name:<20} {note_str}")

print()

if passed == total:
    print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼App è¿è¡Œæ­£å¸¸ï¼")
    exit_code = 0
else:
    print("âš ï¸  éƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯ã€‚")
    exit_code = 1

print("=" * 80)
print()

# é€€å‡ºä»£ç 
sys.exit(exit_code)
